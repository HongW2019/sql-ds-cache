{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nUsage Scenarios\n\n\nArchitecture\n\n\nFeatures\n\n\n\n\nIntroduction\n\n\nApache Spark\n is a unified analytics engine for large-scale data processing, and Spark SQL\n is a Spark module widely used to process structured data in data center. However, Spark SQL still faces the challenge of stability and performance in a highly dynamic environment with ultra-large-scale data.\n\n\nSQL Index and Data Source Cache are designed to leverage users' defined indices and smart fine-grained in-memory data caching to boost Spark SQL performance, and can address the performance issues of some use cases.\n\n\n\n\nUsage Scenarios\n\n\nUsage Scenario 1 -- Interactive queries\n\n\nMost customers adopt Spark SQL as a batch processing engine. Unfortunately, customers would find it hard to separate batch processing and interactive use cases. Interactive queries need to return the data in seconds or even sub-seconds instead of the minutes or hours of batch processing. This is a big challenge for the current Spark SQL implementation.\n\n\nFor example, the following interactive query attempts to filter out a very small result set from a huge fact table.\n\n\nselect ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_cdemo_sk, ss_store_sk, ss_ticket_number, ss_ext_discount_amt, ss_ext_wholesale_cost, ss_ext_tax\n    from fact.ss_sales\n    where (date='20200801' and ss_customer='xxx' and ss_item_sk='806486\u2019)\n    limit 10\n\n\n\n\nInteractive queries usually process a large data set but return a small portion of data filtering for a specific condition. By creating and storing a full B+ Tree index for key columns and using a smart fine-grained in-memory data caching strategy, we can boost Spark SQL interactive queries to seconds and even sub-seconds.\n\n\nBy properly using index and cache, the performance of some interactive queries can possibly be improved by order of magnitude.\n\n\nUsage Scenario 2 -- Batch processing jobs\n\n\nCustomers usually use Spark SQL for Business Analytics in Data Warehousing. SQL Data Source Cache can speed up batch processing jobs with 2 cache strategies:\n\n\n\n\nAutomatically cache hot data.\n\n\nSpecifically cache hot tables. \n\n\n\n\nUsers can choose either strategy based on their need.\n\n\nArchitecture\n\n\nThe following diagram shows the design architecture.\n\n\n\n\nSQL Index and Data Source Cache acts as a \n.jar\n plug-in for Spark SQL.\n\n\n\n\n\n\nWe designed the compatible adapter layer for three columnar storage file formats: \n\n\n\n\n\n\nParquet\n\n\n\n\nORC\n\n\noap(Parquet-like file format defined by OAP).\n\n\n\n\nSQL Index and Data Source Cache have a \nUnified Cache Representation\n for different columnar storage formats and a fine-grained cache unit for one column of a RowGroup.\n\n\n\n\n2 major optimization functions (indexing and caching) are based on unified representation and the adapter. \n\n\nIndices can be created on one or multiple columns of a data file. \n\n\n\n\nData Source Cache can cache both decompressed and decoded vectorized data and binary raw data. Generally, the server's DRAM is used as the cache medium. \nPMem\n can also be used as the cache medium as it will provide a more cost effective solution for the requirements of a high performance environment.\n\n\n\n\n\n\nBoth indexing and caching as \nOptimizer \n Execution\n are transparent for users. See the \nFeatures\n section for details.\n\n\n\n\n\n\nSpark \nThriftServer\n* is a good use case for OAP, because ThriftServer launches Spark Applications which can cache hot data for a long time in the background, and it also accepts query requests from different clients at the same time. \nbin/spark-sql\n, \nbin/spark-shell\n or \nbin/pyspark\n can certainly be used by SQL Index and Data Source Cache, but usually only for interactive test situations.\n\n\n\n\n\n\nFeatures\n\n\nUse indexing and caching to improve Spark SQL performance on ad-hoc queries and batch processing jobs.\n\n\nIndexing\n\n\nUsers can use SQL DDL(create/drop/refresh/check/show index) to use indexing. Once users create indices using DDL, index files are generated in a specific directory and mainly composed of index data and statistics. When queries are executed, analyzing index files to boost performance is transparent to users.\n\n\n\n\n\n\nBTREE, BITMAP Index is an optimization that is widely used in traditional databases. We also adopt these two index types in the project. BTREE indexing is intended for datasets that have a lot of distinct values, and are distributed randomly, such as telephone numbers or ID numbers. BitMap index is intended for datasets with a limited total amount of distinct values, such as state or age.\n\n\n\n\n\n\nStatistics are located in the index file after all the index data are written into the index file. Sometimes, reading indices could bring extra cost for some queries. So we also support 4 statistics (MinMax, Bloom Filter, SampleBase and PartByValue) to help filter. With statistics, we can make sure we only use indices if we can possibly improve the execution.\n\n\n\n\n\n\nCaching\n\n\nCaching is another core feature of OAP. It is also transparent to users. Data Source Cache can automatically load frequently queried (hot) data, and evict data automatically according to the LRU policy when cache is full. Data Source Cache has the following characteristics:\n\n\n\n\nOff-Heap memory. The Data Source Cache uses off-heap memory and avoids the JVM GC. It can also use \nPMem\n as high-performance, high-capacity, low-cost memory\n\n\nCache-Locality. Data Source Cache can schedule computing tasks to the executor which holds needed data in cache, by implementing a cache aware mechanism based on Spark driver and executors communication.\n\n\nCache granularity. A column in one RowGroup (equivalent to Stripe in ORC) of a column-oriented storage format file is loaded into a basic cache unit which is called a \"Fiber\" in OAP.\n\n\nCache Eviction. Data Source Cache cache eviction uses LRU policy, and automatically caches and evicts data transparently to end user.\n\n\nCache configured tables. Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables.\n\n\n\n\n*Other names and brands may be claimed as the property of others.", 
            "title": "Overview"
        }, 
        {
            "location": "/#introduction", 
            "text": "Apache Spark  is a unified analytics engine for large-scale data processing, and Spark SQL  is a Spark module widely used to process structured data in data center. However, Spark SQL still faces the challenge of stability and performance in a highly dynamic environment with ultra-large-scale data.  SQL Index and Data Source Cache are designed to leverage users' defined indices and smart fine-grained in-memory data caching to boost Spark SQL performance, and can address the performance issues of some use cases.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#usage-scenarios", 
            "text": "", 
            "title": "Usage Scenarios"
        }, 
        {
            "location": "/#usage-scenario-1-interactive-queries", 
            "text": "Most customers adopt Spark SQL as a batch processing engine. Unfortunately, customers would find it hard to separate batch processing and interactive use cases. Interactive queries need to return the data in seconds or even sub-seconds instead of the minutes or hours of batch processing. This is a big challenge for the current Spark SQL implementation.  For example, the following interactive query attempts to filter out a very small result set from a huge fact table.  select ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_cdemo_sk, ss_store_sk, ss_ticket_number, ss_ext_discount_amt, ss_ext_wholesale_cost, ss_ext_tax\n    from fact.ss_sales\n    where (date='20200801' and ss_customer='xxx' and ss_item_sk='806486\u2019)\n    limit 10  Interactive queries usually process a large data set but return a small portion of data filtering for a specific condition. By creating and storing a full B+ Tree index for key columns and using a smart fine-grained in-memory data caching strategy, we can boost Spark SQL interactive queries to seconds and even sub-seconds.  By properly using index and cache, the performance of some interactive queries can possibly be improved by order of magnitude.", 
            "title": "Usage Scenario 1 -- Interactive queries"
        }, 
        {
            "location": "/#usage-scenario-2-batch-processing-jobs", 
            "text": "Customers usually use Spark SQL for Business Analytics in Data Warehousing. SQL Data Source Cache can speed up batch processing jobs with 2 cache strategies:   Automatically cache hot data.  Specifically cache hot tables.    Users can choose either strategy based on their need.", 
            "title": "Usage Scenario 2 -- Batch processing jobs"
        }, 
        {
            "location": "/#architecture", 
            "text": "The following diagram shows the design architecture.   SQL Index and Data Source Cache acts as a  .jar  plug-in for Spark SQL.    We designed the compatible adapter layer for three columnar storage file formats:     Parquet   ORC  oap(Parquet-like file format defined by OAP).   SQL Index and Data Source Cache have a  Unified Cache Representation  for different columnar storage formats and a fine-grained cache unit for one column of a RowGroup.   2 major optimization functions (indexing and caching) are based on unified representation and the adapter.   Indices can be created on one or multiple columns of a data file.    Data Source Cache can cache both decompressed and decoded vectorized data and binary raw data. Generally, the server's DRAM is used as the cache medium.  PMem  can also be used as the cache medium as it will provide a more cost effective solution for the requirements of a high performance environment.    Both indexing and caching as  Optimizer   Execution  are transparent for users. See the  Features  section for details.    Spark  ThriftServer * is a good use case for OAP, because ThriftServer launches Spark Applications which can cache hot data for a long time in the background, and it also accepts query requests from different clients at the same time.  bin/spark-sql ,  bin/spark-shell  or  bin/pyspark  can certainly be used by SQL Index and Data Source Cache, but usually only for interactive test situations.", 
            "title": "Architecture"
        }, 
        {
            "location": "/#features", 
            "text": "Use indexing and caching to improve Spark SQL performance on ad-hoc queries and batch processing jobs.", 
            "title": "Features"
        }, 
        {
            "location": "/#indexing", 
            "text": "Users can use SQL DDL(create/drop/refresh/check/show index) to use indexing. Once users create indices using DDL, index files are generated in a specific directory and mainly composed of index data and statistics. When queries are executed, analyzing index files to boost performance is transparent to users.    BTREE, BITMAP Index is an optimization that is widely used in traditional databases. We also adopt these two index types in the project. BTREE indexing is intended for datasets that have a lot of distinct values, and are distributed randomly, such as telephone numbers or ID numbers. BitMap index is intended for datasets with a limited total amount of distinct values, such as state or age.    Statistics are located in the index file after all the index data are written into the index file. Sometimes, reading indices could bring extra cost for some queries. So we also support 4 statistics (MinMax, Bloom Filter, SampleBase and PartByValue) to help filter. With statistics, we can make sure we only use indices if we can possibly improve the execution.", 
            "title": "Indexing"
        }, 
        {
            "location": "/#caching", 
            "text": "Caching is another core feature of OAP. It is also transparent to users. Data Source Cache can automatically load frequently queried (hot) data, and evict data automatically according to the LRU policy when cache is full. Data Source Cache has the following characteristics:   Off-Heap memory. The Data Source Cache uses off-heap memory and avoids the JVM GC. It can also use  PMem  as high-performance, high-capacity, low-cost memory  Cache-Locality. Data Source Cache can schedule computing tasks to the executor which holds needed data in cache, by implementing a cache aware mechanism based on Spark driver and executors communication.  Cache granularity. A column in one RowGroup (equivalent to Stripe in ORC) of a column-oriented storage format file is loaded into a basic cache unit which is called a \"Fiber\" in OAP.  Cache Eviction. Data Source Cache cache eviction uses LRU policy, and automatically caches and evicts data transparently to end user.  Cache configured tables. Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables.", 
            "title": "Caching"
        }, 
        {
            "location": "/#42other-names-and-brands-may-be-claimed-as-the-property-of-others", 
            "text": "", 
            "title": "*Other names and brands may be claimed as the property of others."
        }, 
        {
            "location": "/User-Guide/", 
            "text": "User Guide\n\n\n\n\nPrerequisites\n\n\nGetting Started\n\n\nConfiguration for YARN Cluster Mode\n\n\nConfiguration for Spark Standalone Mode\n\n\nWorking with SQL Index\n\n\nWorking with SQL Data Source Cache\n\n\nRun TPC-DS Benchmark\n\n\nAdvanced Configuration\n\n\n\n\nPrerequisites\n\n\nSQL Index and Data Source Cache on Spark 3.0.0 requires a working Hadoop cluster with YARN and Spark. Running Spark on YARN requires a binary distribution of Spark, which is built with YARN support.\n\n\nGetting Started\n\n\nBuilding\n\n\nWe have provided a Conda package which will automatically install dependencies and build OAP jars, please follow \nOAP-Installation-Guide\n and you can find compiled OAP jars under\n \n$HOME/miniconda2/envs/oapenv/oap_jars\n once finished the installation.\n\n\nIf you\u2019d like to build from source code, please refer to \nDeveloper Guide\n for the detailed steps.\n\n\nSpark Configurations\n\n\nUsers usually test and run Spark SQL or Scala scripts in Spark Shell,  which launches Spark applications on YRAN with \nclient\n mode. In this section, we will start with Spark Shell then introduce other use scenarios. \n\n\nBefore you run \n$SPARK_HOME/bin/spark-shell\n, you need to configure Spark for integration. You need to add or update the following configurations in the Spark configuration file \n$SPARK_HOME/conf/spark-defaults.conf\n on your working node.\n\n\nspark.sql.extensions              org.apache.spark.sql.OapExtensions\n# absolute path of the jar on your working node\nspark.files                       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-\nversion\n-with-spark-\nversion\n.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-\nversion\n-with-spark-\nversion\n.jar\n# relative path to spark.files, just specify jar name in current dir\nspark.executor.extraClassPath     ./oap-cache-\nversion\n-with-spark-\nversion\n.jar:./oap-common-\nversion\n-with-spark-\nversion\n.jar\n# absolute path of the jar on your working node\nspark.driver.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-\nversion\n-with-spark-\nversion\n.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-\nversion\n-with-spark-\nversion\n.jar\n\n\n\n\nVerify Integration\n\n\nAfter configuration, you can follow these steps to verify the OAP integration is working using Spark Shell.\n\n\n\n\nCreate a test data path on your HDFS. \nhdfs:///user/oap/\n for example.\n\n\n\n\n   hadoop fs -mkdir /user/oap/\n\n\n\n\n\n\nLaunch Spark Shell using the following command on your working node.\n\n\n\n\n   $SPARK_HOME/bin/spark-shell\n\n\n\n\n\n\nExecute the following commands in Spark Shell to test OAP integration. \n\n\n\n\n\n    \n spark.sql(s\nCREATE TABLE oap_test (a INT, b STRING)\n          USING parquet\n          OPTIONS (path 'hdfs:///user/oap/')\n.stripMargin)\n    \n val data = (1 to 30000).map { i =\n (i, s\nthis is test $i\n) }.toDF().createOrReplaceTempView(\nt\n)\n    \n spark.sql(\ninsert overwrite table oap_test select * from t\n)\n    \n spark.sql(\ncreate oindex index1 on oap_test (a)\n)\n    \n spark.sql(\nshow oindex from oap_test\n).show()\n\n\n\n\nThis test creates an index for a table and then shows it. If there are no errors, the OAP \n.jar\n is working with the configuration. The picture below is an example of a successfully run.\n\n\n\n\nConfiguration for YARN Cluster Mode\n\n\nSpark Shell, Spark SQL CLI and Thrift Sever run Spark application in \nclient\n mode. While Spark Submit tool can run Spark application in \nclient\n or \ncluster\n mode, which is decided by \n--deploy-mode\n parameter. \nGetting Started\n session has shown the configurations needed for \nclient\n mode. If you are running Spark Submit tool in \ncluster\n mode, you need to follow the below configuration steps instead.\n\n\nAdd the following OAP configuration settings to \n$SPARK_HOME/conf/spark-defaults.conf\n on your working node before running \nspark-submit\n in \ncluster\n mode.\n\n\nspark.sql.extensions              org.apache.spark.sql.OapExtensions\n# absolute path on your working node\nspark.files                       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-\nversion\n-with-spark-\nversion\n.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-\nversion\n-with-spark-\nversion\n.jar\n# relative path to spark.files, just specify jar name in current dir   \nspark.executor.extraClassPath     ./oap-cache-\nversion\n-with-spark-\nversion\n.jar:./oap-common-\nversion\n-with-spark-\nversion\n.jar\n# relative path to spark.files, just specify jar name in current dir\nspark.driver.extraClassPath       ./oap-cache-\nversion\n-with-spark-\nversion\n.jar:./oap-common-\nversion\n-with-spark-\nversion\n.jar\n\n\n\n\nConfiguration for Spark Standalone Mode\n\n\nIn addition to running on the YARN cluster manager, Spark also provides a simple standalone deploy mode. If you are using Spark in Spark Standalone mode:\n\n\n\n\nMake sure the OAP \n.jar\n at the same path of \nall\n the worker nodes.\n\n\nAdd the following configuration settings to \n$SPARK_HOME/conf/spark-defaults.conf\n on the working node.\n\n\n\n\nspark.sql.extensions               org.apache.spark.sql.OapExtensions\n# absolute path on worker nodes\nspark.executor.extraClassPath      $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-\nversion\n-with-spark-\nversion\n.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-\nversion\n-with-spark-\nversion\n.jar\n# absolute path on worker nodes\nspark.driver.extraClassPath        $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-\nversion\n-with-spark-\nversion\n.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-\nversion\n-with-spark-\nversion\n.jar\n\n\n\n\nWorking with SQL Index\n\n\nAfter a successful OAP integration, you can use OAP SQL DDL to manage table indexes. The DDL operations include \nindex create\n, \ndrop\n, \nrefresh\n, and \nshow\n. Test these functions using the following examples in Spark Shell.\n\n\n spark.sql(s\nCREATE TABLE oap_test (a INT, b STRING)\n       USING parquet\n       OPTIONS (path 'hdfs:///user/oap/')\n.stripMargin)\n\n val data = (1 to 30000).map { i =\n (i, s\nthis is test $i\n) }.toDF().createOrReplaceTempView(\nt\n)\n\n spark.sql(\ninsert overwrite table oap_test select * from t\n)       \n\n\n\n\nIndex Creation\n\n\nUse the CREATE OINDEX DDL command to create a B+ Tree index or bitmap index. \n\n\nCREATE OINDEX index_name ON table_name (column_name) USING [BTREE, BITMAP]\n\n\n\n\nThe following example creates a B+ Tree index on column \"a\" of the \noap_test\n table.\n\n\n spark.sql(\ncreate oindex index1 on oap_test (a)\n)\n\n\n\n\nUse SHOW OINDEX command to show all the created indexes on a specified table.\n\n\n spark.sql(\nshow oindex from oap_test\n).show()\n\n\n\n\nUse Index\n\n\nUsing index in a query is transparent. When SQL queries have filter conditions on the column(s) which can take advantage of the index to filter the data scan, the index will automatically be applied to the execution of Spark SQL. The following example will automatically use the underlayer index created on column \"a\".\n\n\n spark.sql(\nSELECT * FROM oap_test WHERE a = 1\n).show()\n\n\n\n\nDrop index\n\n\nUse DROP OINDEX command to drop a named index.\n\n\n spark.sql(\ndrop oindex index1 on oap_test\n)\n\n\n\n\nWorking with SQL Data Source Cache\n\n\nData Source Cache can provide input data cache functionality to the executor. When using the cache data among different SQL queries, configure cache to allow different SQL queries to use the same executor process. Do this by running your queries through the Spark ThriftServer as shown below. For cache media, we support both DRAM and Intel PMem which means you can choose to cache data in DRAM or Intel PMem if you have PMem configured in hardware.\n\n\nUse DRAM Cache\n\n\n\n\nMake the following configuration changes in Spark configuration file \n$SPARK_HOME/conf/spark-defaults.conf\n. \n\n\n\n\n   spark.memory.offHeap.enabled                      false\n   spark.oap.cache.strategy                          guava\n   spark.sql.oap.cache.memory.manager                offheap\n   # according to the resource of cluster\n   spark.executor.memoryOverhead                     50g\n   # equal to the size of executor.memoryOverhead\n   spark.executor.sql.oap.cache.offheap.memory.size  50g\n   # for parquet fileformat, enable binary cache\n   spark.sql.oap.parquet.binary.cache.enabled        true\n   # for orc fileformat, enable binary cache\n   spark.sql.oap.orc.binary.cache.enabled            true\n\n\n\n\nNOTE\n: Change \nspark.executor.sql.oap.cache.offheap.memory.size\n based on the availability of DRAM capacity to cache data, and its size is equal to \nspark.executor.memoryOverhead\n\n\n\n\nLaunch Spark \nThriftServer\n\n\n\n\nLaunch Spark Thrift Server, and use the Beeline command line tool to connect to the Thrift Server to execute DDL or DML operations. The data cache will automatically take effect for Parquet or ORC file sources. \n\n\nThe rest of this section will show you how to do a quick verification of cache functionality. It will reuse the database metastore created in the \nWorking with SQL Index\n section, which creates the \noap_test\n table definition. In production, Spark Thrift Server will have its own metastore database directory or metastore service and use DDL's through Beeline for creating your tables.\n\n\nWhen you run \nspark-shell\n to create the \noap_test\n table, \nmetastore_db\n will be created in the directory where you ran '$SPARK_HOME/bin/spark-shell'. \nGo to that directory\n and execute the following command to launch Thrift JDBC server and run queries.\n\n\n   $SPARK_HOME/sbin/start-thriftserver.sh\n\n\n\n\n\n\nUse Beeline and connect to the Thrift JDBC server, replacing the hostname (mythriftserver) with your own Thrift Server hostname.\n\n\n\n\n   . $SPARK_HOME/bin/beeline -u jdbc:hive2://\nmythriftserver\n:10000       \n\n\n\n\nAfter the connection is established, execute the following commands to check the metastore is initialized correctly.\n\n\n   \n SHOW databases;\n   \n USE default;\n   \n SHOW tables;\n\n\n\n\n\n\nRun queries on the table that will use the cache automatically. For example,\n\n\n\n\n   \n SELECT * FROM oap_test WHERE a = 1;\n   \n SELECT * FROM oap_test WHERE a = 2;\n   \n SELECT * FROM oap_test WHERE a = 3;\n   ...\n\n\n\n\n\n\nOpen the Spark History Web UI and go to the OAP tab page to verify the cache metrics. The following picture is an example.\n\n\n\n\n\n\nUse PMem Cache\n\n\nPrerequisites\n\n\nThe following steps are required to configure OAP to use PMem cache with \nexternal\n cache strategy.\n\n\n\n\n\n\nPMem hardware is successfully deployed on each node in cluster.\n\n\n\n\n\n\nBesides, when enabling SQL Data Source Cache with external cache using Plasma, PMem could get noticeable performance gain with BIOS configuration settings below, especially on cross socket write path.\n\n\n\n\n\n\nSocket Configuration -\n Memory Configuration -\n NGN Configuration -\n Snoopy mode for AD : Enabled\nSocket Configuration -\n Intel UPI General Configuration -\n Stale AtoS :  Disabled\n\n\n\n\n\n\nIt's strongly advised to use \nLinux device mapper\n to interleave PMem across sockets and get maximum size for Plasma.\n\n\n\n\n   // use ipmctl command to show topology and dimm info of PMem\n   ipmctl show -topology\n   ipmctl show -dimm\n   // provision PMem in app direct mode\n   ipmctl create -goal PersistentMemoryType=AppDirect\n   // reboot system to make configuration take affect\n   reboot\n   // check capacity provisioned for app direct mode(AppDirectCapacity)\n   ipmctl show -memoryresources\n   // show the PMem region information\n   ipmctl show -region\n   // create namespace based on the region, multi namespaces can be created on a single region\n   ndctl create-namespace -m fsdax -r region0\n   ndctl create-namespace -m fsdax -r region1\n   // show the created namespaces\n   fdisk -l\n   // create and mount file system\n   sudo dmsetup create striped-pmem\n   mkfs.ext4 -b 4096 -E stride=512 -F /dev/mapper/striped-pmem\n   mkdir -p /mnt/pmem\n   mount -o dax /dev/mapper/striped-pmem /mnt/pmem\n\n\n\n\nFor more information you can refer to \nQuick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory\n\n\n\n\n\n\nSQL Data Source Cache uses Plasma as a node-level external cache service, the benefit of using external cache is data could be shared across process boundaries.  \nPlasma\n is a high-performance shared-memory object store and a component of \nApache Arrow\n. We have modified Plasma to support PMem, and make it open source on \nIntel-bigdata Arrow\n repo. If you have finished \nOAP Installation Guide\n, Plasma will be automatically installed and then you just need copy \narrow-plasma-0.17.0.jar\n to \n$SPARK_HOME/jars\n. For manual building and installation steps you can refer to \nPlasma installation\n.\n\n\n\n\n\n\nRefer to configuration below to apply external cache strategy and start Plasma service on each node and start your workload.\n\n\n\n\n\n\nConfiguration for NUMA\n\n\nInstall \nnumactl\n to bind the executor to the PMem device on the same NUMA node. \n\n\nyum install numactl -y\n\n\nWe recommend you use NUMA-patched Spark to achieve better performance gain for the \nexternal\n strategy compared with Community Spark.\n\nBuild Spark from source to enable NUMA-binding support, refer to \nEnabling-NUMA-binding-for-PMem-in-Spark\n. \n\n\nConfiguration for enabling PMem cache\n\n\nAdd the following configuration to \n$SPARK_HOME/conf/spark-defaults.conf\n.\n\n\n# 2x number of your worker nodes\nspark.executor.instances          6\n# enable numa\nspark.yarn.numa.enabled           true\n# enable SQL Index and Data Source Cache extension in Spark\nspark.sql.extensions              org.apache.spark.sql.OapExtensions\n\n# absolute path of the jar on your working node, when in Yarn client mode\nspark.files                       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-\nversion\n-with-spark-\nversion\n.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-\nversion\n-with-spark-\nversion\n.jar\n# relative path to spark.files, just specify jar name in current dir, when in Yarn client mode\nspark.executor.extraClassPath     ./oap-cache-\nversion\n-with-spark-\nversion\n.jar:./oap-common-\nversion\n-with-spark-\nversion\n.jar\n# absolute path of the jar on your working node,when in Yarn client mode\nspark.driver.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-\nversion\n-with-spark-\nversion\n.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-\nversion\n-with-spark-\nversion\n.jar\n\n# for parquet file format, enable binary cache\nspark.sql.oap.parquet.binary.cache.enabled                   true\n# for ORC file format, enable binary cache\nspark.sql.oap.orc.binary.cache.enabled                       true\n# enable external cache strategy \nspark.oap.cache.strategy                                     external \nspark.sql.oap.dcpmm.free.wait.threshold                      50000000000\n# according to your executor core number\nspark.executor.sql.oap.cache.external.client.pool.size       10\n\n\n\n\nStart Plasma service manually\n\n\nPlasma config parameters:  \n\n\n -m  how much Bytes share memory Plasma will use\n -s  Unix Domain sockcet path\n -d  PMem directory\n\n\n\n\nStart Plasma service on each node with following command, then run your workload. If you install OAP by Conda, you can find \nplasma-store-server\n in the path \n$HOME/miniconda2/envs/oapenv/bin/\n.\n\n\n./plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem  \n\n\n\n\nRemember to kill \nplasma-store-server\n process if you no longer need cache, and you should delete \n/tmp/plasmaStore\n which is a Unix domain socket.  \n\n\n\n\nUse Yarn to start Plamsa service\n\nWhen using Yarn(Hadoop version \n= 3.1) to start Plasma service, you should provide a json file as below.\n\n\n\n\n{\n  \nname\n: \nplasma-store-service\n,\n  \nversion\n: 1,\n  \ncomponents\n :\n  [\n   {\n     \nname\n: \nplasma-store-service\n,\n     \nnumber_of_containers\n: 3,\n     \nlaunch_command\n: \nplasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem\n,\n     \nresource\n: {\n       \ncpus\n: 1,\n       \nmemory\n: 512\n     }\n   }\n  ]\n}\n\n\n\n\nRun command  \nyarn app -launch plasma-store-service /tmp/plasmaLaunch.json\n to start Plasma server.\n\nRun \nyarn app -stop plasma-store-service\n to stop it.\n\nRun \nyarn app -destroy plasma-store-service\nto destroy it.\n\n\nVerify PMem cache functionality\n\n\n\n\n\n\nAfter finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the \nUse DRAM Cache\n guide to verify that cache is working correctly.\n\n\n\n\n\n\nCheck PMem cache size by checking disk space with \ndf -h\n.\n\n\n\n\n\n\nRun TPC-DS Benchmark\n\n\nThis section provides instructions and tools for running TPC-DS queries to evaluate the cache performance of various configurations. The TPC-DS suite has many queries and we select 9 I/O intensive queries to simplify performance evaluation.\n\n\nWe created some tool scripts \noap-benchmark-tool.zip\n to simplify running the workload. If you are already familiar with TPC-DS data generation and running a TPC-DS tool suite, skip our tool and use the TPC-DS tool suite directly.\n\n\nPrerequisites\n\n\n\n\nPython 2.7+ is required on the working node. \n\n\n\n\nPrepare the Tool\n\n\n\n\nDownload \noap-benchmark-tool.zip\n and unzip to a folder (for example, \noap-benchmark-tool\n folder) on your working node. \n\n\nCopy \noap-benchmark-tool/tools/tpcds-kits\n to \nALL\n worker nodes under the same folder (for example, \n/home/oap/tpcds-kits\n).\n\n\n\n\nGenerate TPC-DS Data\n\n\n\n\n\n\nUpdate the values for the following variables in \noap-benchmark-tool/scripts/tool.conf\n based on your environment and needs.\n\n\n\n\n\n\nSPARK_HOME: Point to the Spark home directory of your Spark setup.\n\n\n\n\nTPCDS_KITS_DIR: The tpcds-kits directory you coped to the worker nodes in the above prepare process. For example, /home/oap/tpcds-kits\n\n\nNAMENODE_ADDRESS: Your HDFS Namenode address in the format of host:port.\n\n\nTHRIFT_SERVER_ADDRESS: Your working node address on which you will run Thrift Server.\n\n\nDATA_SCALE: The data scale to be generated in GB\n\n\nDATA_FORMAT: The data file format. You can specify parquet or orc\n\n\n\n\nFor example:\n\n\nexport SPARK_HOME=/home/oap/spark-3.0.0\nexport TPCDS_KITS_DIR=/home/oap/tpcds-kits\nexport NAMENODE_ADDRESS=mynamenode:9000\nexport THRIFT_SERVER_ADDRESS=mythriftserver\nexport DATA_SCALE=1024\nexport DATA_FORMAT=parquet\n\n\n\n\n\n\nStart data generation.\n\n\n\n\nIn the root directory of this tool (\noap-benchmark-tool\n), run \nscripts/run_gen_data.sh\n to start the data generation process. \n\n\ncd oap-benchmark-tool\nsh ./scripts/run_gen_data.sh\n\n\n\n\nOnce finished, the \n$scale\n data will be generated in the HDFS folder \ngenData$scale\n. And a database called \ntpcds_$format$scale\n will contain the TPC-DS tables.\n\n\nStart Spark Thrift Server\n\n\nStart the Thrift Server in the tool root folder, which is the same folder you run data generation scripts. Use either the PMem or DRAM script to start the Thrift Server.\n\n\nUse PMem as Cache Media\n\n\nUpdate the configuration values in \nscripts/spark_thrift_server_yarn_with_PMem.sh\n to reflect your environment. \nNormally, you need to update the following configuration values to cache to PMem.\n\n\n\n\n--num-executors\n\n\n--driver-memory\n\n\n--executor-memory\n\n\n--executor-cores\n\n\n--conf spark.oap.cache.strategy\n\n\n--conf spark.sql.oap.dcpmm.free.wait.threshold\n\n\n--conf spark.executor.sql.oap.cache.external.client.pool.size\n\n\n\n\nThese settings will override the values specified in Spark configuration file ( \nspark-defaults.conf\n). After the configuration is done, you can execute the following command to start Thrift Server.\n\n\ncd oap-benchmark-tool\nsh ./scripts/spark_thrift_server_yarn_with_PMem.sh start\n\n\n\n\nIn this script, we use \nexternal\n as cache strategy for Parquet Binary data cache. \n\n\nUse DRAM as Cache Media\n\n\nUpdate the configuration values in \nscripts/spark_thrift_server_yarn_with_DRAM.sh\n to reflect your environment. Normally, you need to update the following configuration values to cache to DRAM.\n\n\n\n\n--num-executors\n\n\n--driver-memory\n\n\n--executor-memory\n\n\n--executor-cores\n\n\n--conf spark.executor.sql.oap.cache.offheap.memory.size\n\n\n--conf spark.executor.memoryOverhead\n\n\n\n\nThese settings will override the values specified in Spark configuration file (\nspark-defaults.conf\n). After the configuration is done, you can execute the following command to start Thrift Server.\n\n\ncd oap-benchmark-tool\nsh ./scripts/spark_thrift_server_yarn_with_DRAM.sh  start\n\n\n\n\nRun Queries\n\n\nExecute the following command to start to run queries. If you use \nexternal\n cache strategy, also need start plasma service manually as above.\n\n\ncd oap-benchmark-tool\nsh ./scripts/run_tpcds.sh\n\n\n\n\nWhen all the queries are done, you will see the \nresult.json\n file in the current directory. You will find in the 2nd and 3rd round, cache feature takes effect and query time becomes less.\nAnd the Spark webUI OAP tab has more specific OAP cache metrics just as \nsection\n step 5.\n\n\nAdvanced Configuration\n\n\n\n\nAdditional Cache Strategies\n  \n\n\n\n\nIn addition to \nexternal\n cache strategy, SQL Data Source Cache also supports 3 other cache strategies: \nguava\n, \nnoevict\n  and \nvmemcache\n.\n\n\n\n\nIndex and Data Cache Separation\n \n\n\n\n\nTo optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem.\n\n\n\n\nCache Hot Tables\n \n\n\n\n\nData Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables.\n\n\n\n\nColumn Vector Cache\n \n\n\n\n\nThis document above uses \nbinary\n cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time.\n\n\n\n\nLarge Scale and Heterogeneous Cluster Support\n \n\n\n\n\nIntroduce an external database to store cache locality info to support large-scale and heterogeneous clusters.\n\n\nFor more information and configuration details, please refer to \nAdvanced Configuration\n.", 
            "title": "User Guide"
        }, 
        {
            "location": "/User-Guide/#user-guide", 
            "text": "Prerequisites  Getting Started  Configuration for YARN Cluster Mode  Configuration for Spark Standalone Mode  Working with SQL Index  Working with SQL Data Source Cache  Run TPC-DS Benchmark  Advanced Configuration", 
            "title": "User Guide"
        }, 
        {
            "location": "/User-Guide/#prerequisites", 
            "text": "SQL Index and Data Source Cache on Spark 3.0.0 requires a working Hadoop cluster with YARN and Spark. Running Spark on YARN requires a binary distribution of Spark, which is built with YARN support.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/User-Guide/#getting-started", 
            "text": "", 
            "title": "Getting Started"
        }, 
        {
            "location": "/User-Guide/#building", 
            "text": "We have provided a Conda package which will automatically install dependencies and build OAP jars, please follow  OAP-Installation-Guide  and you can find compiled OAP jars under\n  $HOME/miniconda2/envs/oapenv/oap_jars  once finished the installation.  If you\u2019d like to build from source code, please refer to  Developer Guide  for the detailed steps.", 
            "title": "Building"
        }, 
        {
            "location": "/User-Guide/#spark-configurations", 
            "text": "Users usually test and run Spark SQL or Scala scripts in Spark Shell,  which launches Spark applications on YRAN with  client  mode. In this section, we will start with Spark Shell then introduce other use scenarios.   Before you run  $SPARK_HOME/bin/spark-shell , you need to configure Spark for integration. You need to add or update the following configurations in the Spark configuration file  $SPARK_HOME/conf/spark-defaults.conf  on your working node.  spark.sql.extensions              org.apache.spark.sql.OapExtensions\n# absolute path of the jar on your working node\nspark.files                       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache- version -with-spark- version .jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common- version -with-spark- version .jar\n# relative path to spark.files, just specify jar name in current dir\nspark.executor.extraClassPath     ./oap-cache- version -with-spark- version .jar:./oap-common- version -with-spark- version .jar\n# absolute path of the jar on your working node\nspark.driver.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache- version -with-spark- version .jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common- version -with-spark- version .jar", 
            "title": "Spark Configurations"
        }, 
        {
            "location": "/User-Guide/#verify-integration", 
            "text": "After configuration, you can follow these steps to verify the OAP integration is working using Spark Shell.   Create a test data path on your HDFS.  hdfs:///user/oap/  for example.      hadoop fs -mkdir /user/oap/   Launch Spark Shell using the following command on your working node.      $SPARK_HOME/bin/spark-shell   Execute the following commands in Spark Shell to test OAP integration.    \n      spark.sql(s CREATE TABLE oap_test (a INT, b STRING)\n          USING parquet\n          OPTIONS (path 'hdfs:///user/oap/') .stripMargin)\n      val data = (1 to 30000).map { i =  (i, s this is test $i ) }.toDF().createOrReplaceTempView( t )\n      spark.sql( insert overwrite table oap_test select * from t )\n      spark.sql( create oindex index1 on oap_test (a) )\n      spark.sql( show oindex from oap_test ).show()  This test creates an index for a table and then shows it. If there are no errors, the OAP  .jar  is working with the configuration. The picture below is an example of a successfully run.", 
            "title": "Verify Integration"
        }, 
        {
            "location": "/User-Guide/#configuration-for-yarn-cluster-mode", 
            "text": "Spark Shell, Spark SQL CLI and Thrift Sever run Spark application in  client  mode. While Spark Submit tool can run Spark application in  client  or  cluster  mode, which is decided by  --deploy-mode  parameter.  Getting Started  session has shown the configurations needed for  client  mode. If you are running Spark Submit tool in  cluster  mode, you need to follow the below configuration steps instead.  Add the following OAP configuration settings to  $SPARK_HOME/conf/spark-defaults.conf  on your working node before running  spark-submit  in  cluster  mode.  spark.sql.extensions              org.apache.spark.sql.OapExtensions\n# absolute path on your working node\nspark.files                       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache- version -with-spark- version .jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common- version -with-spark- version .jar\n# relative path to spark.files, just specify jar name in current dir   \nspark.executor.extraClassPath     ./oap-cache- version -with-spark- version .jar:./oap-common- version -with-spark- version .jar\n# relative path to spark.files, just specify jar name in current dir\nspark.driver.extraClassPath       ./oap-cache- version -with-spark- version .jar:./oap-common- version -with-spark- version .jar", 
            "title": "Configuration for YARN Cluster Mode"
        }, 
        {
            "location": "/User-Guide/#configuration-for-spark-standalone-mode", 
            "text": "In addition to running on the YARN cluster manager, Spark also provides a simple standalone deploy mode. If you are using Spark in Spark Standalone mode:   Make sure the OAP  .jar  at the same path of  all  the worker nodes.  Add the following configuration settings to  $SPARK_HOME/conf/spark-defaults.conf  on the working node.   spark.sql.extensions               org.apache.spark.sql.OapExtensions\n# absolute path on worker nodes\nspark.executor.extraClassPath      $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache- version -with-spark- version .jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common- version -with-spark- version .jar\n# absolute path on worker nodes\nspark.driver.extraClassPath        $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache- version -with-spark- version .jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common- version -with-spark- version .jar", 
            "title": "Configuration for Spark Standalone Mode"
        }, 
        {
            "location": "/User-Guide/#working-with-sql-index", 
            "text": "After a successful OAP integration, you can use OAP SQL DDL to manage table indexes. The DDL operations include  index create ,  drop ,  refresh , and  show . Test these functions using the following examples in Spark Shell.   spark.sql(s CREATE TABLE oap_test (a INT, b STRING)\n       USING parquet\n       OPTIONS (path 'hdfs:///user/oap/') .stripMargin)  val data = (1 to 30000).map { i =  (i, s this is test $i ) }.toDF().createOrReplaceTempView( t )  spark.sql( insert overwrite table oap_test select * from t )", 
            "title": "Working with SQL Index"
        }, 
        {
            "location": "/User-Guide/#index-creation", 
            "text": "Use the CREATE OINDEX DDL command to create a B+ Tree index or bitmap index.   CREATE OINDEX index_name ON table_name (column_name) USING [BTREE, BITMAP]  The following example creates a B+ Tree index on column \"a\" of the  oap_test  table.   spark.sql( create oindex index1 on oap_test (a) )  Use SHOW OINDEX command to show all the created indexes on a specified table.   spark.sql( show oindex from oap_test ).show()", 
            "title": "Index Creation"
        }, 
        {
            "location": "/User-Guide/#use-index", 
            "text": "Using index in a query is transparent. When SQL queries have filter conditions on the column(s) which can take advantage of the index to filter the data scan, the index will automatically be applied to the execution of Spark SQL. The following example will automatically use the underlayer index created on column \"a\".   spark.sql( SELECT * FROM oap_test WHERE a = 1 ).show()", 
            "title": "Use Index"
        }, 
        {
            "location": "/User-Guide/#drop-index", 
            "text": "Use DROP OINDEX command to drop a named index.   spark.sql( drop oindex index1 on oap_test )", 
            "title": "Drop index"
        }, 
        {
            "location": "/User-Guide/#working-with-sql-data-source-cache", 
            "text": "Data Source Cache can provide input data cache functionality to the executor. When using the cache data among different SQL queries, configure cache to allow different SQL queries to use the same executor process. Do this by running your queries through the Spark ThriftServer as shown below. For cache media, we support both DRAM and Intel PMem which means you can choose to cache data in DRAM or Intel PMem if you have PMem configured in hardware.", 
            "title": "Working with SQL Data Source Cache"
        }, 
        {
            "location": "/User-Guide/#use-dram-cache", 
            "text": "Make the following configuration changes in Spark configuration file  $SPARK_HOME/conf/spark-defaults.conf .       spark.memory.offHeap.enabled                      false\n   spark.oap.cache.strategy                          guava\n   spark.sql.oap.cache.memory.manager                offheap\n   # according to the resource of cluster\n   spark.executor.memoryOverhead                     50g\n   # equal to the size of executor.memoryOverhead\n   spark.executor.sql.oap.cache.offheap.memory.size  50g\n   # for parquet fileformat, enable binary cache\n   spark.sql.oap.parquet.binary.cache.enabled        true\n   # for orc fileformat, enable binary cache\n   spark.sql.oap.orc.binary.cache.enabled            true  NOTE : Change  spark.executor.sql.oap.cache.offheap.memory.size  based on the availability of DRAM capacity to cache data, and its size is equal to  spark.executor.memoryOverhead   Launch Spark  ThriftServer   Launch Spark Thrift Server, and use the Beeline command line tool to connect to the Thrift Server to execute DDL or DML operations. The data cache will automatically take effect for Parquet or ORC file sources.   The rest of this section will show you how to do a quick verification of cache functionality. It will reuse the database metastore created in the  Working with SQL Index  section, which creates the  oap_test  table definition. In production, Spark Thrift Server will have its own metastore database directory or metastore service and use DDL's through Beeline for creating your tables.  When you run  spark-shell  to create the  oap_test  table,  metastore_db  will be created in the directory where you ran '$SPARK_HOME/bin/spark-shell'.  Go to that directory  and execute the following command to launch Thrift JDBC server and run queries.     $SPARK_HOME/sbin/start-thriftserver.sh   Use Beeline and connect to the Thrift JDBC server, replacing the hostname (mythriftserver) with your own Thrift Server hostname.      . $SPARK_HOME/bin/beeline -u jdbc:hive2:// mythriftserver :10000         After the connection is established, execute the following commands to check the metastore is initialized correctly.       SHOW databases;\n     USE default;\n     SHOW tables;   Run queries on the table that will use the cache automatically. For example,        SELECT * FROM oap_test WHERE a = 1;\n     SELECT * FROM oap_test WHERE a = 2;\n     SELECT * FROM oap_test WHERE a = 3;\n   ...   Open the Spark History Web UI and go to the OAP tab page to verify the cache metrics. The following picture is an example.", 
            "title": "Use DRAM Cache"
        }, 
        {
            "location": "/User-Guide/#use-pmem-cache", 
            "text": "", 
            "title": "Use PMem Cache"
        }, 
        {
            "location": "/User-Guide/#prerequisites_1", 
            "text": "The following steps are required to configure OAP to use PMem cache with  external  cache strategy.    PMem hardware is successfully deployed on each node in cluster.    Besides, when enabling SQL Data Source Cache with external cache using Plasma, PMem could get noticeable performance gain with BIOS configuration settings below, especially on cross socket write path.    Socket Configuration -  Memory Configuration -  NGN Configuration -  Snoopy mode for AD : Enabled\nSocket Configuration -  Intel UPI General Configuration -  Stale AtoS :  Disabled   It's strongly advised to use  Linux device mapper  to interleave PMem across sockets and get maximum size for Plasma.      // use ipmctl command to show topology and dimm info of PMem\n   ipmctl show -topology\n   ipmctl show -dimm\n   // provision PMem in app direct mode\n   ipmctl create -goal PersistentMemoryType=AppDirect\n   // reboot system to make configuration take affect\n   reboot\n   // check capacity provisioned for app direct mode(AppDirectCapacity)\n   ipmctl show -memoryresources\n   // show the PMem region information\n   ipmctl show -region\n   // create namespace based on the region, multi namespaces can be created on a single region\n   ndctl create-namespace -m fsdax -r region0\n   ndctl create-namespace -m fsdax -r region1\n   // show the created namespaces\n   fdisk -l\n   // create and mount file system\n   sudo dmsetup create striped-pmem\n   mkfs.ext4 -b 4096 -E stride=512 -F /dev/mapper/striped-pmem\n   mkdir -p /mnt/pmem\n   mount -o dax /dev/mapper/striped-pmem /mnt/pmem  For more information you can refer to  Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory    SQL Data Source Cache uses Plasma as a node-level external cache service, the benefit of using external cache is data could be shared across process boundaries.   Plasma  is a high-performance shared-memory object store and a component of  Apache Arrow . We have modified Plasma to support PMem, and make it open source on  Intel-bigdata Arrow  repo. If you have finished  OAP Installation Guide , Plasma will be automatically installed and then you just need copy  arrow-plasma-0.17.0.jar  to  $SPARK_HOME/jars . For manual building and installation steps you can refer to  Plasma installation .    Refer to configuration below to apply external cache strategy and start Plasma service on each node and start your workload.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/User-Guide/#configuration-for-numa", 
            "text": "Install  numactl  to bind the executor to the PMem device on the same NUMA node.   yum install numactl -y  We recommend you use NUMA-patched Spark to achieve better performance gain for the  external  strategy compared with Community Spark. \nBuild Spark from source to enable NUMA-binding support, refer to  Enabling-NUMA-binding-for-PMem-in-Spark .", 
            "title": "Configuration for NUMA"
        }, 
        {
            "location": "/User-Guide/#configuration-for-enabling-pmem-cache", 
            "text": "Add the following configuration to  $SPARK_HOME/conf/spark-defaults.conf .  # 2x number of your worker nodes\nspark.executor.instances          6\n# enable numa\nspark.yarn.numa.enabled           true\n# enable SQL Index and Data Source Cache extension in Spark\nspark.sql.extensions              org.apache.spark.sql.OapExtensions\n\n# absolute path of the jar on your working node, when in Yarn client mode\nspark.files                       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache- version -with-spark- version .jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common- version -with-spark- version .jar\n# relative path to spark.files, just specify jar name in current dir, when in Yarn client mode\nspark.executor.extraClassPath     ./oap-cache- version -with-spark- version .jar:./oap-common- version -with-spark- version .jar\n# absolute path of the jar on your working node,when in Yarn client mode\nspark.driver.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache- version -with-spark- version .jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common- version -with-spark- version .jar\n\n# for parquet file format, enable binary cache\nspark.sql.oap.parquet.binary.cache.enabled                   true\n# for ORC file format, enable binary cache\nspark.sql.oap.orc.binary.cache.enabled                       true\n# enable external cache strategy \nspark.oap.cache.strategy                                     external \nspark.sql.oap.dcpmm.free.wait.threshold                      50000000000\n# according to your executor core number\nspark.executor.sql.oap.cache.external.client.pool.size       10  Start Plasma service manually  Plasma config parameters:     -m  how much Bytes share memory Plasma will use\n -s  Unix Domain sockcet path\n -d  PMem directory  Start Plasma service on each node with following command, then run your workload. If you install OAP by Conda, you can find  plasma-store-server  in the path  $HOME/miniconda2/envs/oapenv/bin/ .  ./plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem    Remember to kill  plasma-store-server  process if you no longer need cache, and you should delete  /tmp/plasmaStore  which is a Unix domain socket.     Use Yarn to start Plamsa service \nWhen using Yarn(Hadoop version  = 3.1) to start Plasma service, you should provide a json file as below.   {\n   name :  plasma-store-service ,\n   version : 1,\n   components  :\n  [\n   {\n      name :  plasma-store-service ,\n      number_of_containers : 3,\n      launch_command :  plasma-store-server -m 15000000000 -s /tmp/plasmaStore -d /mnt/pmem ,\n      resource : {\n        cpus : 1,\n        memory : 512\n     }\n   }\n  ]\n}  Run command   yarn app -launch plasma-store-service /tmp/plasmaLaunch.json  to start Plasma server. \nRun  yarn app -stop plasma-store-service  to stop it. \nRun  yarn app -destroy plasma-store-service to destroy it.", 
            "title": "Configuration for enabling PMem cache"
        }, 
        {
            "location": "/User-Guide/#verify-pmem-cache-functionality", 
            "text": "After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the  Use DRAM Cache  guide to verify that cache is working correctly.    Check PMem cache size by checking disk space with  df -h .", 
            "title": "Verify PMem cache functionality"
        }, 
        {
            "location": "/User-Guide/#run-tpc-ds-benchmark", 
            "text": "This section provides instructions and tools for running TPC-DS queries to evaluate the cache performance of various configurations. The TPC-DS suite has many queries and we select 9 I/O intensive queries to simplify performance evaluation.  We created some tool scripts  oap-benchmark-tool.zip  to simplify running the workload. If you are already familiar with TPC-DS data generation and running a TPC-DS tool suite, skip our tool and use the TPC-DS tool suite directly.", 
            "title": "Run TPC-DS Benchmark"
        }, 
        {
            "location": "/User-Guide/#prerequisites_2", 
            "text": "Python 2.7+ is required on the working node.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/User-Guide/#prepare-the-tool", 
            "text": "Download  oap-benchmark-tool.zip  and unzip to a folder (for example,  oap-benchmark-tool  folder) on your working node.   Copy  oap-benchmark-tool/tools/tpcds-kits  to  ALL  worker nodes under the same folder (for example,  /home/oap/tpcds-kits ).", 
            "title": "Prepare the Tool"
        }, 
        {
            "location": "/User-Guide/#generate-tpc-ds-data", 
            "text": "Update the values for the following variables in  oap-benchmark-tool/scripts/tool.conf  based on your environment and needs.    SPARK_HOME: Point to the Spark home directory of your Spark setup.   TPCDS_KITS_DIR: The tpcds-kits directory you coped to the worker nodes in the above prepare process. For example, /home/oap/tpcds-kits  NAMENODE_ADDRESS: Your HDFS Namenode address in the format of host:port.  THRIFT_SERVER_ADDRESS: Your working node address on which you will run Thrift Server.  DATA_SCALE: The data scale to be generated in GB  DATA_FORMAT: The data file format. You can specify parquet or orc   For example:  export SPARK_HOME=/home/oap/spark-3.0.0\nexport TPCDS_KITS_DIR=/home/oap/tpcds-kits\nexport NAMENODE_ADDRESS=mynamenode:9000\nexport THRIFT_SERVER_ADDRESS=mythriftserver\nexport DATA_SCALE=1024\nexport DATA_FORMAT=parquet   Start data generation.   In the root directory of this tool ( oap-benchmark-tool ), run  scripts/run_gen_data.sh  to start the data generation process.   cd oap-benchmark-tool\nsh ./scripts/run_gen_data.sh  Once finished, the  $scale  data will be generated in the HDFS folder  genData$scale . And a database called  tpcds_$format$scale  will contain the TPC-DS tables.", 
            "title": "Generate TPC-DS Data"
        }, 
        {
            "location": "/User-Guide/#start-spark-thrift-server", 
            "text": "Start the Thrift Server in the tool root folder, which is the same folder you run data generation scripts. Use either the PMem or DRAM script to start the Thrift Server.", 
            "title": "Start Spark Thrift Server"
        }, 
        {
            "location": "/User-Guide/#use-pmem-as-cache-media", 
            "text": "Update the configuration values in  scripts/spark_thrift_server_yarn_with_PMem.sh  to reflect your environment. \nNormally, you need to update the following configuration values to cache to PMem.   --num-executors  --driver-memory  --executor-memory  --executor-cores  --conf spark.oap.cache.strategy  --conf spark.sql.oap.dcpmm.free.wait.threshold  --conf spark.executor.sql.oap.cache.external.client.pool.size   These settings will override the values specified in Spark configuration file (  spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server.  cd oap-benchmark-tool\nsh ./scripts/spark_thrift_server_yarn_with_PMem.sh start  In this script, we use  external  as cache strategy for Parquet Binary data cache.", 
            "title": "Use PMem as Cache Media"
        }, 
        {
            "location": "/User-Guide/#use-dram-as-cache-media", 
            "text": "Update the configuration values in  scripts/spark_thrift_server_yarn_with_DRAM.sh  to reflect your environment. Normally, you need to update the following configuration values to cache to DRAM.   --num-executors  --driver-memory  --executor-memory  --executor-cores  --conf spark.executor.sql.oap.cache.offheap.memory.size  --conf spark.executor.memoryOverhead   These settings will override the values specified in Spark configuration file ( spark-defaults.conf ). After the configuration is done, you can execute the following command to start Thrift Server.  cd oap-benchmark-tool\nsh ./scripts/spark_thrift_server_yarn_with_DRAM.sh  start", 
            "title": "Use DRAM as Cache Media"
        }, 
        {
            "location": "/User-Guide/#run-queries", 
            "text": "Execute the following command to start to run queries. If you use  external  cache strategy, also need start plasma service manually as above.  cd oap-benchmark-tool\nsh ./scripts/run_tpcds.sh  When all the queries are done, you will see the  result.json  file in the current directory. You will find in the 2nd and 3rd round, cache feature takes effect and query time becomes less.\nAnd the Spark webUI OAP tab has more specific OAP cache metrics just as  section  step 5.", 
            "title": "Run Queries"
        }, 
        {
            "location": "/User-Guide/#advanced-configuration", 
            "text": "Additional Cache Strategies      In addition to  external  cache strategy, SQL Data Source Cache also supports 3 other cache strategies:  guava ,  noevict   and  vmemcache .   Index and Data Cache Separation     To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem.   Cache Hot Tables     Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables.   Column Vector Cache     This document above uses  binary  cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time.   Large Scale and Heterogeneous Cluster Support     Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters.  For more information and configuration details, please refer to  Advanced Configuration .", 
            "title": "Advanced Configuration"
        }, 
        {
            "location": "/Advanced-Configuration/", 
            "text": "Advanced Configuration\n\n\nIn addition to usage information provided in \nUser Guide\n, we provide more strategies for SQL Index and Data Source Cache in this section.\n\n\nTheir needed dependencies like \nMemkind\n ,\nVmemcache\n and \nPlasma\n can be automatically installed when following \nOAP Installation Guide\n, corresponding feature jars can be found under \n$HOME/miniconda2/envs/oapenv/oap_jars\n.\n\n\n\n\nAdditional Cache Strategies\n  In addition to \nexternal\n cache strategy, SQL Data Source Cache also supports 3 other cache strategies: \nguava\n, \nnoevict\n  and \nvmemcache\n.\n\n\nIndex and Data Cache Separation\n  To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem.\n\n\nCache Hot Tables\n  Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables.\n\n\nColumn Vector Cache\n  This document above uses \nbinary\n cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time.\n\n\nLarge Scale and Heterogeneous Cluster Support\n Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters.\n\n\n\n\nAdditional Cache Strategies\n\n\nFollowing table shows features of 4 cache strategies on PMem.\n\n\n\n\n\n\n\n\nguava\n\n\nnoevict\n\n\nvmemcache\n\n\nexternal cache\n\n\n\n\n\n\n\n\n\n\nUse memkind lib to operate on PMem and guava cache strategy when data eviction happens.\n\n\nUse memkind lib to operate on PMem and doesn't allow data eviction.\n\n\nUse vmemcache lib to operate on PMem and LRU cache strategy when data eviction happens.\n\n\nUse Plasma/dlmalloc to operate on PMem and LRU cache strategy when data eviction happens.\n\n\n\n\n\n\nNeed numa patch in Spark for better performance.\n\n\nNeed numa patch in Spark for better performance.\n\n\nNeed numa patch in Spark for better performance.\n\n\nDoesn't need numa patch.\n\n\n\n\n\n\nSuggest using 2 executors one node to keep aligned with PMem paths and numa nodes number.\n\n\nSuggest using 2 executors one node to keep aligned with PMem paths and numa nodes number.\n\n\nSuggest using 2 executors one node to keep aligned with PMem paths and numa nodes number.\n\n\nNode-level cache so there are no limitation for executor number.\n\n\n\n\n\n\nCache data cleaned once executors exited.\n\n\nCache data cleaned once executors exited.\n\n\nCache data cleaned once executors exited.\n\n\nNo data loss when executors exit thus is friendly to dynamic allocation. But currently it has performance overhead than other cache solutions.\n\n\n\n\n\n\n\n\n\n\n\n\nFor cache solution \nguava/noevict\n, make sure \nMemkind\n library installed on every cluster worker node. If you have finished \nOAP Installation Guide\n, libmemkind will be installed. Or manually build and install it following \nmemkind-installation\n, then place \nlibmemkind.so.0\n under \n/lib64/\n on each worker node.\n\n\n\n\n\n\nFor cache solution \nvmemcahe/external\n cache, make sure \nVmemcache\n library has been installed on every cluster worker node. If you have finished \nOAP Installation Guide\n, libvmemcache will be installed. Or you can follow the \nvmemcache-installation\n steps and make sure \nlibvmemcache.so.0\n exist under \n/lib64/\n directory on each worker node.\n\n\n\n\n\n\nIf you have followed \nOAP Installation Guide\n, \nMemkind\n ,\nVmemcache\n and \nPlasma\n will be automatically installed. \nOr you can refer to \nOAP Developer-Guide\n, there is a shell script to help you install these dependencies automatically.\n\n\nUse PMem Cache\n\n\nPrerequisites\n\n\nThe following are required to configure OAP to use PMem cache.\n\n\n\n\n\n\nPMem hardware is successfully deployed on each node in cluster.\n\n\n\n\n\n\nDirectories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as \n/mnt/pmem0\n and \n/mnt/pmem1\n. Correctly installed PMem must be formatted and mounted on every cluster worker node. You can follow these commands to destroy interleaved PMem device which you set in \nUser-Guide\n:\n\n\n\n\n\n\n  # destroy interleaved PMem device which you set when using external cache strategy\n  umount /mnt/pmem\n  dmsetup remove striped-pmem\n  echo y | mkfs.ext4 /dev/pmem0\n  echo y | mkfs.ext4 /dev/pmem1\n  mkdir -p /mnt/pmem0\n  mkdir -p /mnt/pmem1\n  mount -o dax /dev/pmem0 /mnt/pmem0\n  mount -o dax /dev/pmem1 /mnt/pmem1\n\n\n\n\nIn this case file systems are generated for 2 NUMA nodes, which can be checked by \"numactl --hardware\". For a different number of NUMA nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to NUMA nodes.\n\n\nFor more information you can refer to \nQuick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory\n\n\nConfiguration for NUMA\n\n\n\n\nInstall \nnumactl\n to bind the executor to the PMem device on the same NUMA node. \n\n\n\n\nyum install numactl -y\n\n\n\n\nWe strongly recommend you use NUMA-patched Spark to achieve better performance gain for the following 3 cache strategies. Besides, currently using Community Spark occasionally has the problem of two executors being bound to the same PMem path. \n\n\n\n\nBuild Spark from source to enable NUMA-binding support, refer to \nEnabling-NUMA-binding-for-PMem-in-Spark\n. \n\n\nConfiguration for PMem\n\n\nCreate \npersistent-memory.xml\n under \n$SPARK_HOME/conf\n if it doesn't exist. Use the following template and change the \ninitialPath\n to your mounted paths for PMem devices. \n\n\npersistentMemoryPool\n\n  \n!--The numa id--\n\n  \nnumanode id=\n0\n\n    \n!--The initial path for Intel Optane DC persistent memory--\n\n    \ninitialPath\n/mnt/pmem0\n/initialPath\n\n  \n/numanode\n\n  \nnumanode id=\n1\n\n    \ninitialPath\n/mnt/pmem1\n/initialPath\n\n  \n/numanode\n\n\n/persistentMemoryPool\n\n\n\n\n\nGuava cache\n\n\nGuava cache is based on memkind library, built on top of jemalloc and provides memory characteristics. To use it in your workload, follow \nprerequisites\n to set up PMem hardware correctly, also make sure memkind library installed. Then follow configurations below.\n\n\nNOTE\n: \nspark.executor.sql.oap.cache.persistent.memory.reserved.size\n: When we use PMem as memory through memkind library, some portion of the space needs to be reserved for memory management overhead, such as memory segmentation. We suggest reserving 20% - 25% of the available PMem capacity to avoid memory allocation failure. But even with an allocation failure, OAP will continue the operation to read data from original input data and will not cache the data block.\n\n\n# enable numa\nspark.yarn.numa.enabled                                        true\nspark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND                   1\n# for Parquet file format, enable binary cache\nspark.sql.oap.parquet.binary.cache.enabled                     true\n# for ORC file format, enable binary cache\nspark.sql.oap.orc.binary.cache.enabled                         true\n\nspark.sql.oap.cache.memory.manager                             pm \nspark.oap.cache.strategy                                       guava\n# PMem capacity per executor, according to your cluster\nspark.executor.sql.oap.cache.persistent.memory.initial.size    256g\n# Reserved space per executor, according to your cluster\nspark.executor.sql.oap.cache.persistent.memory.reserved.size   50g\n# enable SQL Index and Data Source Cache jar in Spark\nspark.sql.extensions                                           org.apache.spark.sql.OapExtensions\n# absolute path of the jar on your working node, when in Yarn client mode\nspark.files                       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-\nversion\n-with-spark-\nversion\n.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-\nversion\n-with-spark-\nversion\n.jar\n# relative path to spark.files, just specify jar name in current dir, when in Yarn client mode\nspark.executor.extraClassPath     ./oap-cache-\nversion\n-with-spark-\nversion\n.jar:./oap-common-\nversion\n-with-spark-\nversion\n.jar\n# absolute path of the jar on your working node,when in Yarn client mode\nspark.driver.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-\nversion\n-with-spark-\nversion\n.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-\nversion\n-with-spark-\nversion\n.jar\n\n\n\n\n\nMemkind library also support DAX KMEM mode. Refer to \nKernel\n, this chapter will guide how to configure PMem as system RAM. Or \nMemkind support for KMEM DAX option\n for more details.\n\n\nPlease note that DAX KMEM mode need kernel version 5.x and memkind version 1.10 or above. If you choose KMEM mode, change memory manager from \npm\n to \nkmem\n as below.\n\n\nspark.sql.oap.cache.memory.manager           kmem\n\n\n\n\nNoevict cache\n\n\nThe noevict cache strategy is also supported in OAP based on the memkind library for PMem.\n\n\nTo use it in your workload, follow \nprerequisites\n to set up PMem hardware correctly, also make sure memkind library installed. Then follow the configuration below.\n\n\n# enable numa\nspark.yarn.numa.enabled                                      true\nspark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND                 1\n# for Parquet file format, enable binary cache\nspark.sql.oap.parquet.binary.cache.enabled                   true \n# for ORC file format, enable binary cache\nspark.sql.oap.orc.binary.cache.enabled                       true\nspark.oap.cache.strategy                                     noevict \nspark.executor.sql.oap.cache.persistent.memory.initial.size  256g \n\n# Enable OAP extension in Spark\nspark.sql.extensions              org.apache.spark.sql.OapExtensions\n\n# absolute path of the jar on your working node, when in Yarn client mode\nspark.files                       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-\nversion\n-with-spark-\nversion\n.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-\nversion\n-with-spark-\nversion\n.jar\n# relative path to spark.files, just specify jar name in current dir, when in Yarn client mode\nspark.executor.extraClassPath     ./oap-cache-\nversion\n-with-spark-\nversion\n.jar:./oap-common-\nversion\n-with-spark-\nversion\n.jar\n# absolute path of the jar on your working node,when in Yarn client mode\nspark.driver.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-\nversion\n-with-spark-\nversion\n.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-\nversion\n-with-spark-\nversion\n.jar\n\n\n\n\n\nVmemcache\n\n\n\n\nMake sure \nVmemcache\n library has been installed on every cluster worker node if vmemcache strategy is chosen for PMem cache. If you have finished \nOAP-Installation-Guide\n, vmemcache library will be automatically installed by Conda.\n\n\n\n\nOr you can follow the \nbuild/install\n steps and make sure \nlibvmemcache.so\n exist in \n/lib64\n directory on each worker node.\n- To use it in your workload, follow \nprerequisites\n to set up PMem hardware correctly.\n\n\nConfigure to enable PMem cache\n\n\nMake the following configuration changes in \n$SPARK_HOME/conf/spark-defaults.conf\n.\n\n\n# 2x number of your worker nodes\nspark.executor.instances          6\n# enable numa\nspark.yarn.numa.enabled           true\n# Enable OAP extension in Spark\nspark.sql.extensions              org.apache.spark.sql.OapExtensions\n\n# absolute path of the jar on your working node, when in Yarn client mode\nspark.files                       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-\nversion\n-with-spark-\nversion\n.jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-\nversion\n-with-spark-\nversion\n.jar\n# relative path to spark.files, just specify jar name in current dir, when in Yarn client mode\nspark.executor.extraClassPath     ./oap-cache-\nversion\n-with-spark-\nversion\n.jar:./oap-common-\nversion\n-with-spark-\nversion\n.jar\n# absolute path of the jar on your working node,when in Yarn client mode\nspark.driver.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache-\nversion\n-with-spark-\nversion\n.jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common-\nversion\n-with-spark-\nversion\n.jar\n\n# for parquet file format, enable binary cache\nspark.sql.oap.parquet.binary.cache.enabled                   true\n# for ORC file format, enable binary cache\nspark.sql.oap.orc.binary.cache.enabled                       true\n# enable vmemcache strategy \nspark.oap.cache.strategy                                     vmem \nspark.executor.sql.oap.cache.persistent.memory.initial.size  256g \n# according to your cluster\nspark.executor.sql.oap.cache.guardian.memory.size            10g\n\n\n\n\nThe \nvmem\n cache strategy is based on libvmemcache (buffer based LRU cache), which provides a key-value store API. Follow these steps to enable vmemcache support in Data Source Cache.\n\n\n\n\nspark.executor.instances\n: We suggest setting the value to 2X the number of worker nodes when NUMA binding is enabled. Each worker node runs two executors, each executor is bound to one of the two sockets, and accesses the corresponding PMem device on that socket.\n\n\nspark.executor.sql.oap.cache.persistent.memory.initial.size\n: It is configured to the available PMem capacity to be used as data cache per exectutor.\n\n\n\n\nNOTE\n: If \"PendingFiber Size\" (on spark web-UI OAP page) is large, or some tasks fail with \"cache guardian use too much memory\" error, set \nspark.executor.sql.oap.cache.guardian.memory.size\n to a larger number as the default size is 10GB. The user could also increase \nspark.sql.oap.cache.guardian.free.thread.nums\n or decrease \nspark.sql.oap.cache.dispose.timeout.ms\n to free memory more quickly.\n\n\nVerify PMem cache functionality\n\n\n\n\n\n\nAfter finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the \nUse DRAM Cache\n guide to verify that cache is working correctly.\n\n\n\n\n\n\nVerify NUMA binding status by confirming keywords like \nnumactl --cpubind=1 --membind=1\n contained in executor launch command.\n\n\n\n\n\n\nCheck PMem cache size by checking disk space with \ndf -h\n.For \nvmemcache\n strategy, disk usage will reach the initial cache size once the PMem cache is initialized and will not change during workload execution. For \nGuava/Noevict\n strategies, the command will show disk space usage increases along with workload execution. \n\n\n\n\n\n\nIndex and Data Cache Separation\n\n\nSQL Index and Data Source Cache now supports different cache strategies for DRAM and PMem. To optimize the cache media utilization, you can enable cache separation of data and index with same or different cache media. When Sharing same media, data cache and index cache will use different fiber cache ratio.\n\n\nHere we list 4 different kinds of configuration for index/cache separation, if you choose one of them, please add corresponding configuration to \nspark-defaults.conf\n.\n1. DRAM as cache media, \nguava\n strategy as index \n data cache backend. \n\n\nspark.sql.oap.index.data.cache.separation.enabled\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0true\nspark.oap.cache.strategy\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 mix\nspark.sql.oap.cache.memory.manager  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0     offheap\n\n\n\n\nThe rest configuration you can refer to  \nUse DRAM Cache\n \n\n\n\n\nPMem as cache media, \nexternal\n strategy as index \n data cache backend. \n\n\n\n\nspark.sql.oap.index.data.cache.separation.enabled\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0true\nspark.oap.cache.strategy\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 mix\nspark.sql.oap.cache.memory.manager                      tmp\nspark.sql.oap.mix.data.cache.backend                    external\nspark.sql.oap.mix.index.cache.backend                   external\n\n\n\n\n\nThe rest configurations can refer to the configurations of \nPMem Cache\n and  \nExternal cache\n\n\n\n\nDRAM(\noffheap\n)/\nguava\n as \nindex\n cache media and backend, PMem(\ntmp\n)/\nexternal\n as \ndata\n cache media and backend. \n\n\n\n\nspark.sql.oap.index.data.cache.separation.enabled            true\nspark.oap.cache.strategy                                     mix\nspark.sql.oap.cache.memory.manager                           mix \nspark.sql.oap.mix.data.cache.backend                         external\n\n# 2x number of your worker nodes\nspark.executor.instances                                     6\n# enable numa\nspark.yarn.numa.enabled                                      true\nspark.memory.offHeap.enabled                                 false\n\nspark.sql.oap.dcpmm.free.wait.threshold                      50000000000\n# according to your executor core number\nspark.executor.sql.oap.cache.external.client.pool.size       10\n\n# equal to the size of executor.memoryOverhead\nspark.executor.sql.oap.cache.offheap.memory.size             50g\n# according to the resource of cluster\nspark.executor.memoryOverhead                                50g\n\n# for ORC file format\nspark.sql.oap.orc.binary.cache.enabled                       true\n# for Parquet file format\nspark.sql.oap.parquet.binary.cache.enabled                   true\n\n\n\n\n\n\nDRAM(\noffheap\n)/\nguava\n as \nindex\n cache media and backend, PMem(\npm\n)/\nguava\n as \ndata\n cache media and backend. \n\n\n\n\nspark.sql.oap.index.data.cache.separation.enabled            true\nspark.oap.cache.strategy                                     mix\nspark.sql.oap.cache.memory.manager                           mix \n\n# 2x number of your worker nodes\nspark.executor.instances                                     6\n# enable numa\nspark.yarn.numa.enabled                                      true\nspark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND                 1\nspark.memory.offHeap.enabled                                 false\n# PMem capacity per executor\nspark.executor.sql.oap.cache.persistent.memory.initial.size  256g\n# Reserved space per executor\nspark.executor.sql.oap.cache.persistent.memory.reserved.size 50g\n\n# equal to the size of executor.memoryOverhead\nspark.executor.sql.oap.cache.offheap.memory.size             50g\n# according to the resource of cluster\nspark.executor.memoryOverhead                                50g\n# for ORC file format\nspark.sql.oap.orc.binary.cache.enabled                       true\n# for Parquet file format\nspark.sql.oap.parquet.binary.cache.enabled                   true\n\n\n\n\nCache Hot Tables\n\n\nData Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables.\n\n\nTo enable caching specific hot tables, you can add the configuration below to \nspark-defaults.conf\n.\n\n\n# enable table lists fiberCache\nspark.sql.oap.cache.table.list.enabled          true\n# Table lists using fiberCache actively\nspark.sql.oap.cache.table.list                  \ndatabasename\n.\ntablename1\n;\ndatabasename\n.\ntablename2\n\n\n\n\n\nColumn Vector Cache\n\n\nThis document above use \nbinary\n cache for Parquet as example, cause binary cache can improve cache space utilization compared to ColumnVector cache. When your cluster memory resources are abundant enough, you can choose ColumnVector cache to spare computation time. \n\n\nTo enable ColumnVector data cache for Parquet file format, you should add the configuration below to \nspark-defaults.conf\n.\n\n\n# for parquet file format, disable binary cache\nspark.sql.oap.parquet.binary.cache.enabled             false\n# for parquet file format, enable ColumnVector cache\nspark.sql.oap.parquet.data.cache.enabled               true\n\n\n\n\nLarge Scale and Heterogeneous Cluster Support\n\n\nNOTE:\n Only works with \nexternal cache\n\n\nOAP influences Spark to schedule tasks according to cache locality info. This info could be of large amount in a \nlarge scale cluster\n, and how to schedule tasks in a \nheterogeneous cluster\n (some nodes with PMem, some without) could also be challenging.\n\n\nWe introduce an external DB to store cache locality info. If there's no cache available, Spark will fall back to schedule respecting HDFS locality.\nCurrently we support \nRedis\n as external DB service. Please \ndownload and launch a redis-server\n before running Spark with OAP.\n\n\nPlease add the following configurations to \nspark-defaults.conf\n.\n\n\nspark.sql.oap.external.cache.metaDB.enabled            true\n# Redis-server address\nspark.sql.oap.external.cache.metaDB.address            10.1.2.12\nspark.sql.oap.external.cache.metaDB.impl               org.apache.spark.sql.execution.datasources.RedisClient", 
            "title": "Advanced Configuration"
        }, 
        {
            "location": "/Advanced-Configuration/#advanced-configuration", 
            "text": "In addition to usage information provided in  User Guide , we provide more strategies for SQL Index and Data Source Cache in this section.  Their needed dependencies like  Memkind  , Vmemcache  and  Plasma  can be automatically installed when following  OAP Installation Guide , corresponding feature jars can be found under  $HOME/miniconda2/envs/oapenv/oap_jars .   Additional Cache Strategies   In addition to  external  cache strategy, SQL Data Source Cache also supports 3 other cache strategies:  guava ,  noevict   and  vmemcache .  Index and Data Cache Separation   To optimize the cache media utilization, SQL Data Source Cache supports cache separation of data and index, by using same or different cache media with DRAM and PMem.  Cache Hot Tables   Data Source Cache also supports caching specific tables according to actual situations, these tables are usually hot tables.  Column Vector Cache   This document above uses  binary  cache as example for Parquet file format, if your cluster memory resources is abundant enough, you can choose ColumnVector data cache instead of binary cache for Parquet to spare computation time.  Large Scale and Heterogeneous Cluster Support  Introduce an external database to store cache locality info to support large-scale and heterogeneous clusters.", 
            "title": "Advanced Configuration"
        }, 
        {
            "location": "/Advanced-Configuration/#additional-cache-strategies", 
            "text": "Following table shows features of 4 cache strategies on PMem.     guava  noevict  vmemcache  external cache      Use memkind lib to operate on PMem and guava cache strategy when data eviction happens.  Use memkind lib to operate on PMem and doesn't allow data eviction.  Use vmemcache lib to operate on PMem and LRU cache strategy when data eviction happens.  Use Plasma/dlmalloc to operate on PMem and LRU cache strategy when data eviction happens.    Need numa patch in Spark for better performance.  Need numa patch in Spark for better performance.  Need numa patch in Spark for better performance.  Doesn't need numa patch.    Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number.  Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number.  Suggest using 2 executors one node to keep aligned with PMem paths and numa nodes number.  Node-level cache so there are no limitation for executor number.    Cache data cleaned once executors exited.  Cache data cleaned once executors exited.  Cache data cleaned once executors exited.  No data loss when executors exit thus is friendly to dynamic allocation. But currently it has performance overhead than other cache solutions.       For cache solution  guava/noevict , make sure  Memkind  library installed on every cluster worker node. If you have finished  OAP Installation Guide , libmemkind will be installed. Or manually build and install it following  memkind-installation , then place  libmemkind.so.0  under  /lib64/  on each worker node.    For cache solution  vmemcahe/external  cache, make sure  Vmemcache  library has been installed on every cluster worker node. If you have finished  OAP Installation Guide , libvmemcache will be installed. Or you can follow the  vmemcache-installation  steps and make sure  libvmemcache.so.0  exist under  /lib64/  directory on each worker node.    If you have followed  OAP Installation Guide ,  Memkind  , Vmemcache  and  Plasma  will be automatically installed. \nOr you can refer to  OAP Developer-Guide , there is a shell script to help you install these dependencies automatically.", 
            "title": "Additional Cache Strategies"
        }, 
        {
            "location": "/Advanced-Configuration/#use-pmem-cache", 
            "text": "", 
            "title": "Use PMem Cache"
        }, 
        {
            "location": "/Advanced-Configuration/#prerequisites", 
            "text": "The following are required to configure OAP to use PMem cache.    PMem hardware is successfully deployed on each node in cluster.    Directories exposing PMem hardware on each socket. For example, on a two socket system the mounted PMem directories should appear as  /mnt/pmem0  and  /mnt/pmem1 . Correctly installed PMem must be formatted and mounted on every cluster worker node. You can follow these commands to destroy interleaved PMem device which you set in  User-Guide :      # destroy interleaved PMem device which you set when using external cache strategy\n  umount /mnt/pmem\n  dmsetup remove striped-pmem\n  echo y | mkfs.ext4 /dev/pmem0\n  echo y | mkfs.ext4 /dev/pmem1\n  mkdir -p /mnt/pmem0\n  mkdir -p /mnt/pmem1\n  mount -o dax /dev/pmem0 /mnt/pmem0\n  mount -o dax /dev/pmem1 /mnt/pmem1  In this case file systems are generated for 2 NUMA nodes, which can be checked by \"numactl --hardware\". For a different number of NUMA nodes, a corresponding number of namespaces should be created to assure correct file system paths mapping to NUMA nodes.  For more information you can refer to  Quick Start Guide: Provision Intel\u00ae Optane\u2122 DC Persistent Memory", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/Advanced-Configuration/#configuration-for-numa", 
            "text": "Install  numactl  to bind the executor to the PMem device on the same NUMA node.    yum install numactl -y   We strongly recommend you use NUMA-patched Spark to achieve better performance gain for the following 3 cache strategies. Besides, currently using Community Spark occasionally has the problem of two executors being bound to the same PMem path.    Build Spark from source to enable NUMA-binding support, refer to  Enabling-NUMA-binding-for-PMem-in-Spark .", 
            "title": "Configuration for NUMA"
        }, 
        {
            "location": "/Advanced-Configuration/#configuration-for-pmem", 
            "text": "Create  persistent-memory.xml  under  $SPARK_HOME/conf  if it doesn't exist. Use the following template and change the  initialPath  to your mounted paths for PMem devices.   persistentMemoryPool \n   !--The numa id-- \n   numanode id= 0 \n     !--The initial path for Intel Optane DC persistent memory-- \n     initialPath /mnt/pmem0 /initialPath \n   /numanode \n   numanode id= 1 \n     initialPath /mnt/pmem1 /initialPath \n   /numanode  /persistentMemoryPool", 
            "title": "Configuration for PMem"
        }, 
        {
            "location": "/Advanced-Configuration/#guava-cache", 
            "text": "Guava cache is based on memkind library, built on top of jemalloc and provides memory characteristics. To use it in your workload, follow  prerequisites  to set up PMem hardware correctly, also make sure memkind library installed. Then follow configurations below.  NOTE :  spark.executor.sql.oap.cache.persistent.memory.reserved.size : When we use PMem as memory through memkind library, some portion of the space needs to be reserved for memory management overhead, such as memory segmentation. We suggest reserving 20% - 25% of the available PMem capacity to avoid memory allocation failure. But even with an allocation failure, OAP will continue the operation to read data from original input data and will not cache the data block.  # enable numa\nspark.yarn.numa.enabled                                        true\nspark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND                   1\n# for Parquet file format, enable binary cache\nspark.sql.oap.parquet.binary.cache.enabled                     true\n# for ORC file format, enable binary cache\nspark.sql.oap.orc.binary.cache.enabled                         true\n\nspark.sql.oap.cache.memory.manager                             pm \nspark.oap.cache.strategy                                       guava\n# PMem capacity per executor, according to your cluster\nspark.executor.sql.oap.cache.persistent.memory.initial.size    256g\n# Reserved space per executor, according to your cluster\nspark.executor.sql.oap.cache.persistent.memory.reserved.size   50g\n# enable SQL Index and Data Source Cache jar in Spark\nspark.sql.extensions                                           org.apache.spark.sql.OapExtensions\n# absolute path of the jar on your working node, when in Yarn client mode\nspark.files                       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache- version -with-spark- version .jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common- version -with-spark- version .jar\n# relative path to spark.files, just specify jar name in current dir, when in Yarn client mode\nspark.executor.extraClassPath     ./oap-cache- version -with-spark- version .jar:./oap-common- version -with-spark- version .jar\n# absolute path of the jar on your working node,when in Yarn client mode\nspark.driver.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache- version -with-spark- version .jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common- version -with-spark- version .jar  Memkind library also support DAX KMEM mode. Refer to  Kernel , this chapter will guide how to configure PMem as system RAM. Or  Memkind support for KMEM DAX option  for more details.  Please note that DAX KMEM mode need kernel version 5.x and memkind version 1.10 or above. If you choose KMEM mode, change memory manager from  pm  to  kmem  as below.  spark.sql.oap.cache.memory.manager           kmem", 
            "title": "Guava cache"
        }, 
        {
            "location": "/Advanced-Configuration/#noevict-cache", 
            "text": "The noevict cache strategy is also supported in OAP based on the memkind library for PMem.  To use it in your workload, follow  prerequisites  to set up PMem hardware correctly, also make sure memkind library installed. Then follow the configuration below.  # enable numa\nspark.yarn.numa.enabled                                      true\nspark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND                 1\n# for Parquet file format, enable binary cache\nspark.sql.oap.parquet.binary.cache.enabled                   true \n# for ORC file format, enable binary cache\nspark.sql.oap.orc.binary.cache.enabled                       true\nspark.oap.cache.strategy                                     noevict \nspark.executor.sql.oap.cache.persistent.memory.initial.size  256g \n\n# Enable OAP extension in Spark\nspark.sql.extensions              org.apache.spark.sql.OapExtensions\n\n# absolute path of the jar on your working node, when in Yarn client mode\nspark.files                       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache- version -with-spark- version .jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common- version -with-spark- version .jar\n# relative path to spark.files, just specify jar name in current dir, when in Yarn client mode\nspark.executor.extraClassPath     ./oap-cache- version -with-spark- version .jar:./oap-common- version -with-spark- version .jar\n# absolute path of the jar on your working node,when in Yarn client mode\nspark.driver.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache- version -with-spark- version .jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common- version -with-spark- version .jar", 
            "title": "Noevict cache"
        }, 
        {
            "location": "/Advanced-Configuration/#vmemcache", 
            "text": "Make sure  Vmemcache  library has been installed on every cluster worker node if vmemcache strategy is chosen for PMem cache. If you have finished  OAP-Installation-Guide , vmemcache library will be automatically installed by Conda.   Or you can follow the  build/install  steps and make sure  libvmemcache.so  exist in  /lib64  directory on each worker node.\n- To use it in your workload, follow  prerequisites  to set up PMem hardware correctly.", 
            "title": "Vmemcache"
        }, 
        {
            "location": "/Advanced-Configuration/#configure-to-enable-pmem-cache", 
            "text": "Make the following configuration changes in  $SPARK_HOME/conf/spark-defaults.conf .  # 2x number of your worker nodes\nspark.executor.instances          6\n# enable numa\nspark.yarn.numa.enabled           true\n# Enable OAP extension in Spark\nspark.sql.extensions              org.apache.spark.sql.OapExtensions\n\n# absolute path of the jar on your working node, when in Yarn client mode\nspark.files                       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache- version -with-spark- version .jar,$HOME/miniconda2/envs/oapenv/oap_jars/oap-common- version -with-spark- version .jar\n# relative path to spark.files, just specify jar name in current dir, when in Yarn client mode\nspark.executor.extraClassPath     ./oap-cache- version -with-spark- version .jar:./oap-common- version -with-spark- version .jar\n# absolute path of the jar on your working node,when in Yarn client mode\nspark.driver.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/oap-cache- version -with-spark- version .jar:$HOME/miniconda2/envs/oapenv/oap_jars/oap-common- version -with-spark- version .jar\n\n# for parquet file format, enable binary cache\nspark.sql.oap.parquet.binary.cache.enabled                   true\n# for ORC file format, enable binary cache\nspark.sql.oap.orc.binary.cache.enabled                       true\n# enable vmemcache strategy \nspark.oap.cache.strategy                                     vmem \nspark.executor.sql.oap.cache.persistent.memory.initial.size  256g \n# according to your cluster\nspark.executor.sql.oap.cache.guardian.memory.size            10g  The  vmem  cache strategy is based on libvmemcache (buffer based LRU cache), which provides a key-value store API. Follow these steps to enable vmemcache support in Data Source Cache.   spark.executor.instances : We suggest setting the value to 2X the number of worker nodes when NUMA binding is enabled. Each worker node runs two executors, each executor is bound to one of the two sockets, and accesses the corresponding PMem device on that socket.  spark.executor.sql.oap.cache.persistent.memory.initial.size : It is configured to the available PMem capacity to be used as data cache per exectutor.   NOTE : If \"PendingFiber Size\" (on spark web-UI OAP page) is large, or some tasks fail with \"cache guardian use too much memory\" error, set  spark.executor.sql.oap.cache.guardian.memory.size  to a larger number as the default size is 10GB. The user could also increase  spark.sql.oap.cache.guardian.free.thread.nums  or decrease  spark.sql.oap.cache.dispose.timeout.ms  to free memory more quickly.", 
            "title": "Configure to enable PMem cache"
        }, 
        {
            "location": "/Advanced-Configuration/#verify-pmem-cache-functionality", 
            "text": "After finishing configuration, restart Spark Thrift Server for the configuration changes to take effect. Start at step 2 of the  Use DRAM Cache  guide to verify that cache is working correctly.    Verify NUMA binding status by confirming keywords like  numactl --cpubind=1 --membind=1  contained in executor launch command.    Check PMem cache size by checking disk space with  df -h .For  vmemcache  strategy, disk usage will reach the initial cache size once the PMem cache is initialized and will not change during workload execution. For  Guava/Noevict  strategies, the command will show disk space usage increases along with workload execution.", 
            "title": "Verify PMem cache functionality"
        }, 
        {
            "location": "/Advanced-Configuration/#index-and-data-cache-separation", 
            "text": "SQL Index and Data Source Cache now supports different cache strategies for DRAM and PMem. To optimize the cache media utilization, you can enable cache separation of data and index with same or different cache media. When Sharing same media, data cache and index cache will use different fiber cache ratio.  Here we list 4 different kinds of configuration for index/cache separation, if you choose one of them, please add corresponding configuration to  spark-defaults.conf .\n1. DRAM as cache media,  guava  strategy as index   data cache backend.   spark.sql.oap.index.data.cache.separation.enabled\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0true\nspark.oap.cache.strategy\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 mix\nspark.sql.oap.cache.memory.manager  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0     offheap  The rest configuration you can refer to   Use DRAM Cache     PMem as cache media,  external  strategy as index   data cache backend.    spark.sql.oap.index.data.cache.separation.enabled\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0true\nspark.oap.cache.strategy\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 mix\nspark.sql.oap.cache.memory.manager                      tmp\nspark.sql.oap.mix.data.cache.backend                    external\nspark.sql.oap.mix.index.cache.backend                   external  The rest configurations can refer to the configurations of  PMem Cache  and   External cache   DRAM( offheap )/ guava  as  index  cache media and backend, PMem( tmp )/ external  as  data  cache media and backend.    spark.sql.oap.index.data.cache.separation.enabled            true\nspark.oap.cache.strategy                                     mix\nspark.sql.oap.cache.memory.manager                           mix \nspark.sql.oap.mix.data.cache.backend                         external\n\n# 2x number of your worker nodes\nspark.executor.instances                                     6\n# enable numa\nspark.yarn.numa.enabled                                      true\nspark.memory.offHeap.enabled                                 false\n\nspark.sql.oap.dcpmm.free.wait.threshold                      50000000000\n# according to your executor core number\nspark.executor.sql.oap.cache.external.client.pool.size       10\n\n# equal to the size of executor.memoryOverhead\nspark.executor.sql.oap.cache.offheap.memory.size             50g\n# according to the resource of cluster\nspark.executor.memoryOverhead                                50g\n\n# for ORC file format\nspark.sql.oap.orc.binary.cache.enabled                       true\n# for Parquet file format\nspark.sql.oap.parquet.binary.cache.enabled                   true   DRAM( offheap )/ guava  as  index  cache media and backend, PMem( pm )/ guava  as  data  cache media and backend.    spark.sql.oap.index.data.cache.separation.enabled            true\nspark.oap.cache.strategy                                     mix\nspark.sql.oap.cache.memory.manager                           mix \n\n# 2x number of your worker nodes\nspark.executor.instances                                     6\n# enable numa\nspark.yarn.numa.enabled                                      true\nspark.executorEnv.MEMKIND_ARENA_NUM_PER_KIND                 1\nspark.memory.offHeap.enabled                                 false\n# PMem capacity per executor\nspark.executor.sql.oap.cache.persistent.memory.initial.size  256g\n# Reserved space per executor\nspark.executor.sql.oap.cache.persistent.memory.reserved.size 50g\n\n# equal to the size of executor.memoryOverhead\nspark.executor.sql.oap.cache.offheap.memory.size             50g\n# according to the resource of cluster\nspark.executor.memoryOverhead                                50g\n# for ORC file format\nspark.sql.oap.orc.binary.cache.enabled                       true\n# for Parquet file format\nspark.sql.oap.parquet.binary.cache.enabled                   true", 
            "title": "Index and Data Cache Separation"
        }, 
        {
            "location": "/Advanced-Configuration/#cache-hot-tables", 
            "text": "Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables.  To enable caching specific hot tables, you can add the configuration below to  spark-defaults.conf .  # enable table lists fiberCache\nspark.sql.oap.cache.table.list.enabled          true\n# Table lists using fiberCache actively\nspark.sql.oap.cache.table.list                   databasename . tablename1 ; databasename . tablename2", 
            "title": "Cache Hot Tables"
        }, 
        {
            "location": "/Advanced-Configuration/#column-vector-cache", 
            "text": "This document above use  binary  cache for Parquet as example, cause binary cache can improve cache space utilization compared to ColumnVector cache. When your cluster memory resources are abundant enough, you can choose ColumnVector cache to spare computation time.   To enable ColumnVector data cache for Parquet file format, you should add the configuration below to  spark-defaults.conf .  # for parquet file format, disable binary cache\nspark.sql.oap.parquet.binary.cache.enabled             false\n# for parquet file format, enable ColumnVector cache\nspark.sql.oap.parquet.data.cache.enabled               true", 
            "title": "Column Vector Cache"
        }, 
        {
            "location": "/Advanced-Configuration/#large-scale-and-heterogeneous-cluster-support", 
            "text": "NOTE:  Only works with  external cache  OAP influences Spark to schedule tasks according to cache locality info. This info could be of large amount in a  large scale cluster , and how to schedule tasks in a  heterogeneous cluster  (some nodes with PMem, some without) could also be challenging.  We introduce an external DB to store cache locality info. If there's no cache available, Spark will fall back to schedule respecting HDFS locality.\nCurrently we support  Redis  as external DB service. Please  download and launch a redis-server  before running Spark with OAP.  Please add the following configurations to  spark-defaults.conf .  spark.sql.oap.external.cache.metaDB.enabled            true\n# Redis-server address\nspark.sql.oap.external.cache.metaDB.address            10.1.2.12\nspark.sql.oap.external.cache.metaDB.impl               org.apache.spark.sql.execution.datasources.RedisClient", 
            "title": "Large Scale and Heterogeneous Cluster Support"
        }, 
        {
            "location": "/OAP-Installation-Guide/", 
            "text": "OAP Installation Guide\n\n\nThis document introduces how to install OAP and its dependencies on your cluster nodes by \nConda\n. \nFollow steps below on \nevery node\n of your cluster to set right environment for each machine.\n\n\nContents\n\n\n\n\nPrerequisites\n\n\nInstalling OAP\n\n\nConfiguration\n\n\n\n\nPrerequisites\n\n\n\n\n\n\nOS Requirements\n\nWe have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use \nFedora 29 CentOS 7.6 or above\n. Besides, for \nMemkind\n we recommend you use \nkernel above 3.10\n.\n\n\n\n\n\n\nConda Requirements\n \n\nInstall Conda on your cluster nodes with below commands and follow the prompts on the installer screens.:\n\n\n\n\n\n\n$ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh\n$ chmod +x Miniconda2-latest-Linux-x86_64.sh \n$ bash Miniconda2-latest-Linux-x86_64.sh \n\n\n\n\nFor changes to take effect, close and re-open your current shell. To test your installation,  run the command \nconda list\n in your terminal window. A list of installed packages appears if it has been installed correctly.\n\n\nInstalling OAP\n\n\nDependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps.\n\n\n\n\nArrow\n\n\nPlasma\n\n\nMemkind\n\n\nVmemcache\n\n\nHPNL\n\n\nPMDK\n  \n\n\nOneAPI\n\n\n\n\nCreate a conda environment and install OAP Conda package.\n\n\n$ conda create -n oapenv -y python=3.7\n$ conda activate oapenv\n$ conda install -c conda-forge -c intel -y oap=1.0.0\n\n\n\n\nOnce finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under \n$HOME/miniconda2/envs/oapenv/oap_jars\n\n\nExtra Steps for PMem Shuffle\n\n\nIf you use one of OAP features -- \nPMem Shuffle\n with \nRDMA\n, you need to configure and validate RDMA, please refer to \nPMem Shuffle\n for the details.\n\n\nConfiguration\n\n\nOnce finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to \n$SPARK_HOME/conf/spark-defaults.conf\n.\n\n\nspark.executorEnv.LD_LIBRARY_PATH   $HOME/miniconda2/envs/oapenv/lib\nspark.executor.extraLibraryPath     $HOME/miniconda2/envs/oapenv/lib\nspark.driver.extraLibraryPath       $HOME/miniconda2/envs/oapenv/lib\nspark.executor.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar\nspark.driver.extraClassPath         $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar\n\n\n\n\nAnd then you can follow the corresponding feature documents for more details to use them.", 
            "title": "OAP Installation Guide"
        }, 
        {
            "location": "/OAP-Installation-Guide/#oap-installation-guide", 
            "text": "This document introduces how to install OAP and its dependencies on your cluster nodes by  Conda . \nFollow steps below on  every node  of your cluster to set right environment for each machine.", 
            "title": "OAP Installation Guide"
        }, 
        {
            "location": "/OAP-Installation-Guide/#contents", 
            "text": "Prerequisites  Installing OAP  Configuration", 
            "title": "Contents"
        }, 
        {
            "location": "/OAP-Installation-Guide/#prerequisites", 
            "text": "OS Requirements \nWe have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use  Fedora 29 CentOS 7.6 or above . Besides, for  Memkind  we recommend you use  kernel above 3.10 .    Conda Requirements   \nInstall Conda on your cluster nodes with below commands and follow the prompts on the installer screens.:    $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh\n$ chmod +x Miniconda2-latest-Linux-x86_64.sh \n$ bash Miniconda2-latest-Linux-x86_64.sh   For changes to take effect, close and re-open your current shell. To test your installation,  run the command  conda list  in your terminal window. A list of installed packages appears if it has been installed correctly.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/OAP-Installation-Guide/#installing-oap", 
            "text": "Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps.   Arrow  Plasma  Memkind  Vmemcache  HPNL  PMDK     OneAPI   Create a conda environment and install OAP Conda package.  $ conda create -n oapenv -y python=3.7\n$ conda activate oapenv\n$ conda install -c conda-forge -c intel -y oap=1.0.0  Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under  $HOME/miniconda2/envs/oapenv/oap_jars", 
            "title": "Installing OAP"
        }, 
        {
            "location": "/OAP-Installation-Guide/#extra-steps-for-pmem-shuffle", 
            "text": "If you use one of OAP features --  PMem Shuffle  with  RDMA , you need to configure and validate RDMA, please refer to  PMem Shuffle  for the details.", 
            "title": "Extra Steps for PMem Shuffle"
        }, 
        {
            "location": "/OAP-Installation-Guide/#configuration", 
            "text": "Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to  $SPARK_HOME/conf/spark-defaults.conf .  spark.executorEnv.LD_LIBRARY_PATH   $HOME/miniconda2/envs/oapenv/lib\nspark.executor.extraLibraryPath     $HOME/miniconda2/envs/oapenv/lib\nspark.driver.extraLibraryPath       $HOME/miniconda2/envs/oapenv/lib\nspark.executor.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar\nspark.driver.extraClassPath         $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar  And then you can follow the corresponding feature documents for more details to use them.", 
            "title": "Configuration"
        }, 
        {
            "location": "/OAP-Developer-Guide/", 
            "text": "OAP Developer Guide\n\n\nThis document contains the instructions \n scripts on installing necessary dependencies and building OAP. \nYou can get more detailed information from OAP each module below.\n\n\n\n\nSQL DS Cache\n\n\nPMem Common\n\n\nPMem Spill\n\n\nPMem Shuffle\n\n\nRemote Shuffle\n\n\nOAP MLlib\n\n\nArrow Data Source\n\n\nNative SQL Engine\n\n\n\n\nBuilding OAP\n\n\nPrerequisites for Building\n\n\nOAP is built with \nApache Maven\n and Oracle Java 8, and mainly required tools to install on your cluster are listed below.\n\n\n\n\nCmake\n\n\nGCC \n 7\n\n\nMemkind\n\n\nVmemcache\n\n\nHPNL\n\n\nPMDK\n  \n\n\nOneAPI\n\n\n\n\nArrow\n\n\n\n\n\n\nRequirements for PMem Shuffle\n\n\n\n\n\n\nIf enable PMem Shuffle with RDMA, you can refer to \nPMem Shuffle\n to configure and validate RDMA in advance.\n\n\nWe provide scripts below to help automatically install dependencies above \nexcept RDMA\n, need change to \nroot\n account, run:\n\n\n# git clone -b \nversion\n https://github.com/oap-project/oap-tools.git\n# cd oap-tools\n# sh dev/install-compile-time-dependencies.sh\n\n\n\n\nRun the following command to learn more.\n\n\n# sh dev/scripts/prepare_oap_env.sh --help\n\n\n\n\nRun the following command to automatically install specific dependency such as Maven.\n\n\n# sh dev/scripts/prepare_oap_env.sh --prepare_maven\n\n\n\n\nBuilding\n\n\nTo build OAP package, run command below then you can find a tarball named \noap-$VERSION-bin-spark-$VERSION.tar.gz\n under directory \n$OAP_TOOLS_HOME/dev/release-package\n.\n\n\n$ sh $OAP_TOOLS_HOME/dev/compile-oap.sh\n\n\n\n\nBuilding Specified OAP Module, such as \nsql-ds-cache\n, run:\n\n\n$ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache", 
            "title": "OAP Developer Guide"
        }, 
        {
            "location": "/OAP-Developer-Guide/#oap-developer-guide", 
            "text": "This document contains the instructions   scripts on installing necessary dependencies and building OAP. \nYou can get more detailed information from OAP each module below.   SQL DS Cache  PMem Common  PMem Spill  PMem Shuffle  Remote Shuffle  OAP MLlib  Arrow Data Source  Native SQL Engine", 
            "title": "OAP Developer Guide"
        }, 
        {
            "location": "/OAP-Developer-Guide/#building-oap", 
            "text": "", 
            "title": "Building OAP"
        }, 
        {
            "location": "/OAP-Developer-Guide/#prerequisites-for-building", 
            "text": "OAP is built with  Apache Maven  and Oracle Java 8, and mainly required tools to install on your cluster are listed below.   Cmake  GCC   7  Memkind  Vmemcache  HPNL  PMDK     OneAPI   Arrow    Requirements for PMem Shuffle    If enable PMem Shuffle with RDMA, you can refer to  PMem Shuffle  to configure and validate RDMA in advance.  We provide scripts below to help automatically install dependencies above  except RDMA , need change to  root  account, run:  # git clone -b  version  https://github.com/oap-project/oap-tools.git\n# cd oap-tools\n# sh dev/install-compile-time-dependencies.sh  Run the following command to learn more.  # sh dev/scripts/prepare_oap_env.sh --help  Run the following command to automatically install specific dependency such as Maven.  # sh dev/scripts/prepare_oap_env.sh --prepare_maven", 
            "title": "Prerequisites for Building"
        }, 
        {
            "location": "/OAP-Developer-Guide/#building", 
            "text": "To build OAP package, run command below then you can find a tarball named  oap-$VERSION-bin-spark-$VERSION.tar.gz  under directory  $OAP_TOOLS_HOME/dev/release-package .  $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh  Building Specified OAP Module, such as  sql-ds-cache , run:  $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache", 
            "title": "Building"
        }, 
        {
            "location": "/Developer-Guide/", 
            "text": "Developer Guide\n\n\nThis document is a supplement to the whole \nOAP Developer Guide\n for SQL Index and Data Source Cache.\nAfter following that document, you can continue more details for SQL Index and Data Source Cache.\n\n\n\n\nBuilding\n\n\nEnabling NUMA binding for Intel\u00ae Optane\u2122 DC Persistent Memory in Spark\n\n\n\n\nBuilding\n\n\nBuilding SQL Index and  Data Source Cache\n\n\nBuilding with \nApache Maven*\n.\n\n\nClone the OAP project:\n\n\ngit clone -b \ntag-version\n  https://github.com/Intel-bigdata/OAP.git\ncd OAP\n\n\n\n\nBuild the \noap-cache\n package:\n\n\nmvn clean -pl com.intel.oap:oap-cache -am package\n\n\n\n\nRunning Tests\n\n\nRun all the tests:\n\n\nmvn clean -pl com.intel.oap:oap-cache -am test\n\n\n\n\nRun a specific test suite, for example \nOapDDLSuite\n:\n\n\nmvn -pl com.intel.oap:oap-cache -am -DwildcardSuites=org.apache.spark.sql.execution.datasources.oap.OapDDLSuite test\n\n\n\n\nNOTE\n: Log level of unit tests currently default to ERROR, please override oap-cache/oap/src/test/resources/log4j.properties if needed.\n\n\nBuilding with Intel\u00ae Optane\u2122 DC Persistent Memory Module\n\n\nPrerequisites for building with PMem support\n\n\nInstall the required packages on the build system:\n\n\n\n\ncmake\n\n\nmemkind\n\n\nvmemcache\n\n\nPlasma\n\n\n\n\nmemkind installation\n\n\nThe memkind library depends on \nlibnuma\n at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source:\n\n\ngit clone -b v1.10.1 https://github.com/memkind/memkind\ncd memkind\n./autogen.sh\n./configure\nmake\nmake install\n\n\n\n\nvmemcache installation\n\n\nTo build vmemcache library from source, you can (for RPM-based linux as example):\n\n\ngit clone https://github.com/pmem/vmemcache\ncd vmemcache\nmkdir build\ncd build\ncmake .. -DCMAKE_INSTALL_PREFIX=/usr -DCPACK_GENERATOR=rpm\nmake package\nsudo rpm -i libvmemcache*.rpm\n\n\n\n\nPlasma installation\n\n\nTo use optimized Plasma cache with OAP, you need following components:  \n\n\n(1) \nlibarrow.so\n, \nlibplasma.so\n, \nlibplasma_java.so\n: dynamic libraries, will be used in Plasma client. \n\n   (2) \nplasma-store-server\n: executable file, Plasma cache service.\n\n   (3) \narrow-plasma-0.17.0.jar\n: will be used when compile oap and spark runtime also need it. \n\n\n\n\n.so\n file and binary file\n\n  Clone code from Intel-arrow repo and run following commands, this will install \nlibplasma.so\n, \nlibarrow.so\n, \nlibplasma_java.so\n and \nplasma-store-server\n to your system path(\n/usr/lib64\n by default). And if you are using Spark in a cluster environment, you can copy these files to all nodes in your cluster if the OS or distribution are same, otherwise, you need compile it on each node.\n\n\n\n\ncd /tmp\ngit clone https://github.com/Intel-bigdata/arrow.git\ncd arrow \n git checkout branch-0.17.0-oap-1.0\ncd cpp\nmkdir release\ncd release\n#build libarrow, libplasma, libplasma_java\ncmake -DCMAKE_INSTALL_PREFIX=/usr/ -DCMAKE_BUILD_TYPE=Release -DARROW_BUILD_TESTS=on -DARROW_PLASMA_JAVA_CLIENT=on -DARROW_PLASMA=on -DARROW_DEPENDENCY_SOURCE=BUNDLED  ..\nmake -j$(nproc)\nsudo make install -j$(nproc)\n\n\n\n\n\n\narrow-plasma-0.17.0.jar\n\n\narrow-plasma-0.17.0.jar\n is provided in Maven central repo, you can download \nit\n and copy to \n$SPARK_HOME/jars\n dir.\n\n\n\n\nOr you can manually install it, run following command, this will install arrow jars to your local maven repo. Besides, you need copy arrow-plasma-0.17.0.jar to \n$SPARK_HOME/jars/\n dir, cause this jar is needed when using external cache.\n\n\ncd /tmp/arrow/java\nmvn clean -q -pl plasma -DskipTests install\n\n\n\n\nBuilding the package\n\n\nYou need to add \n-Ppersistent-memory\n to build with PMem support. For \nnoevict\n cache strategy, you also need to build with \n-Ppersistent-memory\n parameter.\n\n\nmvn clean -q -pl com.intel.oap:oap-cache -am  -Ppersistent-memory -DskipTests package\n\n\n\n\nFor vmemcache cache strategy, please build with command:\n\n\nmvn clean -q -pl com.intel.oap:oap-cache -am -Pvmemcache -DskipTests package\n\n\n\n\nBuild with this command to use all of them:\n\n\nmvn clean -q -pl com.intel.oap:oap-cache -am  -Ppersistent-memory -Pvmemcache -DskipTests package\n\n\n\n\nEnabling NUMA binding for PMem in Spark\n\n\nRebuilding Spark packages with NUMA binding patch\n\n\nWhen using PMem as a cache medium apply the \nNUMA\n binding patch \nnuma-binding-spark-3.0.0.patch\n to Spark source code for best performance.\n\n\n\n\n\n\nDownload src for \nSpark-3.0.0\n and clone the src from github.\n\n\n\n\n\n\nApply this patch and \nrebuild\n the Spark package.\n\n\n\n\n\n\ngit apply  numa-binding-spark-3.0.0.patch\n\n\n\n\n\n\nAdd these configuration items to the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf to enable NUMA binding.\n\n\n\n\nspark.yarn.numa.enabled true \n\n\n\n\nNOTE\n: If you are using a customized Spark, you will need to manually resolve the conflicts.\n\n\n*Other names and brands may be claimed as the property of others.", 
            "title": "Developer Guide"
        }, 
        {
            "location": "/Developer-Guide/#developer-guide", 
            "text": "This document is a supplement to the whole  OAP Developer Guide  for SQL Index and Data Source Cache.\nAfter following that document, you can continue more details for SQL Index and Data Source Cache.   Building  Enabling NUMA binding for Intel\u00ae Optane\u2122 DC Persistent Memory in Spark", 
            "title": "Developer Guide"
        }, 
        {
            "location": "/Developer-Guide/#building", 
            "text": "", 
            "title": "Building"
        }, 
        {
            "location": "/Developer-Guide/#building-sql-index-and-data-source-cache", 
            "text": "Building with  Apache Maven* .  Clone the OAP project:  git clone -b  tag-version   https://github.com/Intel-bigdata/OAP.git\ncd OAP  Build the  oap-cache  package:  mvn clean -pl com.intel.oap:oap-cache -am package", 
            "title": "Building SQL Index and  Data Source Cache"
        }, 
        {
            "location": "/Developer-Guide/#running-tests", 
            "text": "Run all the tests:  mvn clean -pl com.intel.oap:oap-cache -am test  Run a specific test suite, for example  OapDDLSuite :  mvn -pl com.intel.oap:oap-cache -am -DwildcardSuites=org.apache.spark.sql.execution.datasources.oap.OapDDLSuite test  NOTE : Log level of unit tests currently default to ERROR, please override oap-cache/oap/src/test/resources/log4j.properties if needed.", 
            "title": "Running Tests"
        }, 
        {
            "location": "/Developer-Guide/#building-with-intel-optanetm-dc-persistent-memory-module", 
            "text": "", 
            "title": "Building with Intel\u00ae Optane\u2122 DC Persistent Memory Module"
        }, 
        {
            "location": "/Developer-Guide/#prerequisites-for-building-with-pmem-support", 
            "text": "Install the required packages on the build system:   cmake  memkind  vmemcache  Plasma", 
            "title": "Prerequisites for building with PMem support"
        }, 
        {
            "location": "/Developer-Guide/#memkind-installation", 
            "text": "The memkind library depends on  libnuma  at the runtime, so it must already exist in the worker node system. Build the latest memkind lib from source:  git clone -b v1.10.1 https://github.com/memkind/memkind\ncd memkind\n./autogen.sh\n./configure\nmake\nmake install", 
            "title": "memkind installation"
        }, 
        {
            "location": "/Developer-Guide/#vmemcache-installation", 
            "text": "To build vmemcache library from source, you can (for RPM-based linux as example):  git clone https://github.com/pmem/vmemcache\ncd vmemcache\nmkdir build\ncd build\ncmake .. -DCMAKE_INSTALL_PREFIX=/usr -DCPACK_GENERATOR=rpm\nmake package\nsudo rpm -i libvmemcache*.rpm", 
            "title": "vmemcache installation"
        }, 
        {
            "location": "/Developer-Guide/#plasma-installation", 
            "text": "To use optimized Plasma cache with OAP, you need following components:    (1)  libarrow.so ,  libplasma.so ,  libplasma_java.so : dynamic libraries, will be used in Plasma client.  \n   (2)  plasma-store-server : executable file, Plasma cache service. \n   (3)  arrow-plasma-0.17.0.jar : will be used when compile oap and spark runtime also need it.    .so  file and binary file \n  Clone code from Intel-arrow repo and run following commands, this will install  libplasma.so ,  libarrow.so ,  libplasma_java.so  and  plasma-store-server  to your system path( /usr/lib64  by default). And if you are using Spark in a cluster environment, you can copy these files to all nodes in your cluster if the OS or distribution are same, otherwise, you need compile it on each node.   cd /tmp\ngit clone https://github.com/Intel-bigdata/arrow.git\ncd arrow   git checkout branch-0.17.0-oap-1.0\ncd cpp\nmkdir release\ncd release\n#build libarrow, libplasma, libplasma_java\ncmake -DCMAKE_INSTALL_PREFIX=/usr/ -DCMAKE_BUILD_TYPE=Release -DARROW_BUILD_TESTS=on -DARROW_PLASMA_JAVA_CLIENT=on -DARROW_PLASMA=on -DARROW_DEPENDENCY_SOURCE=BUNDLED  ..\nmake -j$(nproc)\nsudo make install -j$(nproc)   arrow-plasma-0.17.0.jar  arrow-plasma-0.17.0.jar  is provided in Maven central repo, you can download  it  and copy to  $SPARK_HOME/jars  dir.   Or you can manually install it, run following command, this will install arrow jars to your local maven repo. Besides, you need copy arrow-plasma-0.17.0.jar to  $SPARK_HOME/jars/  dir, cause this jar is needed when using external cache.  cd /tmp/arrow/java\nmvn clean -q -pl plasma -DskipTests install", 
            "title": "Plasma installation"
        }, 
        {
            "location": "/Developer-Guide/#building-the-package", 
            "text": "You need to add  -Ppersistent-memory  to build with PMem support. For  noevict  cache strategy, you also need to build with  -Ppersistent-memory  parameter.  mvn clean -q -pl com.intel.oap:oap-cache -am  -Ppersistent-memory -DskipTests package  For vmemcache cache strategy, please build with command:  mvn clean -q -pl com.intel.oap:oap-cache -am -Pvmemcache -DskipTests package  Build with this command to use all of them:  mvn clean -q -pl com.intel.oap:oap-cache -am  -Ppersistent-memory -Pvmemcache -DskipTests package", 
            "title": "Building the package"
        }, 
        {
            "location": "/Developer-Guide/#enabling-numa-binding-for-pmem-in-spark", 
            "text": "", 
            "title": "Enabling NUMA binding for PMem in Spark"
        }, 
        {
            "location": "/Developer-Guide/#rebuilding-spark-packages-with-numa-binding-patch", 
            "text": "When using PMem as a cache medium apply the  NUMA  binding patch  numa-binding-spark-3.0.0.patch  to Spark source code for best performance.    Download src for  Spark-3.0.0  and clone the src from github.    Apply this patch and  rebuild  the Spark package.    git apply  numa-binding-spark-3.0.0.patch   Add these configuration items to the Spark configuration file $SPARK_HOME/conf/spark-defaults.conf to enable NUMA binding.   spark.yarn.numa.enabled true   NOTE : If you are using a customized Spark, you will need to manually resolve the conflicts.", 
            "title": "Rebuilding Spark packages with NUMA binding patch"
        }, 
        {
            "location": "/Developer-Guide/#42other-names-and-brands-may-be-claimed-as-the-property-of-others", 
            "text": "", 
            "title": "*Other names and brands may be claimed as the property of others."
        }
    ]
}