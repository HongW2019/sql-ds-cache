{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\nUsage Scenarios\n\n\nArchitecture\n\n\nFeatures\n\n\n\n\nIntroduction\n\n\nApache Spark\n is a unified analytics engine for large-scale data processing, and Spark SQL\n is a Spark module widely used to process structured data in data center. However, Spark SQL still faces the challenge of stability and performance in a highly dynamic environment with ultra-large-scale data.\n\n\nSQL Index and Data Source Cache are designed to leverage users' defined indices and smart fine-grained in-memory data caching to boost Spark SQL performance, and can address the performance issues of some use cases.\n\n\n\n\nUsage Scenarios\n\n\nUsage Scenario 1 -- Interactive queries\n\n\nMost customers adopt Spark SQL as a batch processing engine. Unfortunately, customers would find it hard to separate batch processing and interactive use cases. Interactive queries need to return the data in seconds or even sub-seconds instead of the minutes or hours of batch processing. This is a big challenge for the current Spark SQL implementation.\n\n\nFor example, the following interactive query attempts to filter out a very small result set from a huge fact table.\n\n\nselect ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_cdemo_sk, ss_store_sk, ss_ticket_number, ss_ext_discount_amt, ss_ext_wholesale_cost, ss_ext_tax\n    from fact.ss_sales\n    where (date='20200801' and ss_customer='xxx' and ss_item_sk='806486\u2019)\n    limit 10\n\n\n\n\nInteractive queries usually process a large data set but return a small portion of data filtering for a specific condition. By creating and storing a full B+ Tree index for key columns and using a smart fine-grained in-memory data caching strategy, we can boost Spark SQL interactive queries to seconds and even sub-seconds.\n\n\nBy properly using index and cache, the performance of some interactive queries can possibly be improved by order of magnitude.\n\n\nUsage Scenario 2 -- Batch processing jobs\n\n\nCustomers usually use Spark SQL for Business Analytics in Data Warehousing. SQL Data Source Cache can speed up batch processing jobs with 2 cache strategies:\n\n\n\n\nAutomatically cache hot data.\n\n\nSpecifically cache hot tables. \n\n\n\n\nUsers can choose either strategy based on their need.\n\n\nArchitecture\n\n\nThe following diagram shows the design architecture.\n\n\n\n\nSQL Index and Data Source Cache acts as a \n.jar\n plug-in for Spark SQL.\n\n\n\n\n\n\nWe designed the compatible adapter layer for three columnar storage file formats: \n\n\n\n\n\n\nParquet\n\n\n\n\nORC\n\n\noap(Parquet-like file format defined by OAP).\n\n\n\n\nSQL Index and Data Source Cache have a \nUnified Cache Representation\n for different columnar storage formats and a fine-grained cache unit for one column of a RowGroup.\n\n\n\n\n2 major optimization functions (indexing and caching) are based on unified representation and the adapter. \n\n\nIndices can be created on one or multiple columns of a data file. \n\n\n\n\nData Source Cache can cache both decompressed and decoded vectorized data and binary raw data. Generally, the server's DRAM is used as the cache medium. \nPMem\n can also be used as the cache medium as it will provide a more cost effective solution for the requirements of a high performance environment.\n\n\n\n\n\n\nBoth indexing and caching as \nOptimizer \n Execution\n are transparent for users. See the \nFeatures\n section for details.\n\n\n\n\n\n\nSpark \nThriftServer\n* is a good use case for OAP, because ThriftServer launches Spark Applications which can cache hot data for a long time in the background, and it also accepts query requests from different clients at the same time. \nbin/spark-sql\n, \nbin/spark-shell\n or \nbin/pyspark\n can certainly be used by SQL Index and Data Source Cache, but usually only for interactive test situations.\n\n\n\n\n\n\nFeatures\n\n\nUse indexing and caching to improve Spark SQL performance on ad-hoc queries and batch processing jobs.\n\n\nIndexing\n\n\nUsers can use SQL DDL(create/drop/refresh/check/show index) to use indexing. Once users create indices using DDL, index files are generated in a specific directory and mainly composed of index data and statistics. When queries are executed, analyzing index files to boost performance is transparent to users.\n\n\n\n\n\n\nBTREE, BITMAP Index is an optimization that is widely used in traditional databases. We also adopt these two index types in the project. BTREE indexing is intended for datasets that have a lot of distinct values, and are distributed randomly, such as telephone numbers or ID numbers. BitMap index is intended for datasets with a limited total amount of distinct values, such as state or age.\n\n\n\n\n\n\nStatistics are located in the index file after all the index data are written into the index file. Sometimes, reading indices could bring extra cost for some queries. So we also support 4 statistics (MinMax, Bloom Filter, SampleBase and PartByValue) to help filter. With statistics, we can make sure we only use indices if we can possibly improve the execution.\n\n\n\n\n\n\nCaching\n\n\nCaching is another core feature of OAP. It is also transparent to users. Data Source Cache can automatically load frequently queried (hot) data, and evict data automatically according to the LRU policy when cache is full. Data Source Cache has the following characteristics:\n\n\n\n\nOff-Heap memory. The Data Source Cache uses off-heap memory and avoids the JVM GC. It can also use \nPMem\n as high-performance, high-capacity, low-cost memory\n\n\nCache-Locality. Data Source Cache can schedule computing tasks to the executor which holds needed data in cache, by implementing a cache aware mechanism based on Spark driver and executors communication.\n\n\nCache granularity. A column in one RowGroup (equivalent to Stripe in ORC) of a column-oriented storage format file is loaded into a basic cache unit which is called a \"Fiber\" in OAP.\n\n\nCache Eviction. Data Source Cache cache eviction uses LRU policy, and automatically caches and evicts data transparently to end user.\n\n\nCache configured tables. Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables.\n\n\n\n\n*Other names and brands may be claimed as the property of others.", 
            "title": "Overview"
        }, 
        {
            "location": "/#introduction", 
            "text": "Apache Spark  is a unified analytics engine for large-scale data processing, and Spark SQL  is a Spark module widely used to process structured data in data center. However, Spark SQL still faces the challenge of stability and performance in a highly dynamic environment with ultra-large-scale data.  SQL Index and Data Source Cache are designed to leverage users' defined indices and smart fine-grained in-memory data caching to boost Spark SQL performance, and can address the performance issues of some use cases.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#usage-scenarios", 
            "text": "", 
            "title": "Usage Scenarios"
        }, 
        {
            "location": "/#usage-scenario-1-interactive-queries", 
            "text": "Most customers adopt Spark SQL as a batch processing engine. Unfortunately, customers would find it hard to separate batch processing and interactive use cases. Interactive queries need to return the data in seconds or even sub-seconds instead of the minutes or hours of batch processing. This is a big challenge for the current Spark SQL implementation.  For example, the following interactive query attempts to filter out a very small result set from a huge fact table.  select ss_sold_date_sk, ss_sold_time_sk, ss_item_sk, ss_cdemo_sk, ss_store_sk, ss_ticket_number, ss_ext_discount_amt, ss_ext_wholesale_cost, ss_ext_tax\n    from fact.ss_sales\n    where (date='20200801' and ss_customer='xxx' and ss_item_sk='806486\u2019)\n    limit 10  Interactive queries usually process a large data set but return a small portion of data filtering for a specific condition. By creating and storing a full B+ Tree index for key columns and using a smart fine-grained in-memory data caching strategy, we can boost Spark SQL interactive queries to seconds and even sub-seconds.  By properly using index and cache, the performance of some interactive queries can possibly be improved by order of magnitude.", 
            "title": "Usage Scenario 1 -- Interactive queries"
        }, 
        {
            "location": "/#usage-scenario-2-batch-processing-jobs", 
            "text": "Customers usually use Spark SQL for Business Analytics in Data Warehousing. SQL Data Source Cache can speed up batch processing jobs with 2 cache strategies:   Automatically cache hot data.  Specifically cache hot tables.    Users can choose either strategy based on their need.", 
            "title": "Usage Scenario 2 -- Batch processing jobs"
        }, 
        {
            "location": "/#architecture", 
            "text": "The following diagram shows the design architecture.   SQL Index and Data Source Cache acts as a  .jar  plug-in for Spark SQL.    We designed the compatible adapter layer for three columnar storage file formats:     Parquet   ORC  oap(Parquet-like file format defined by OAP).   SQL Index and Data Source Cache have a  Unified Cache Representation  for different columnar storage formats and a fine-grained cache unit for one column of a RowGroup.   2 major optimization functions (indexing and caching) are based on unified representation and the adapter.   Indices can be created on one or multiple columns of a data file.    Data Source Cache can cache both decompressed and decoded vectorized data and binary raw data. Generally, the server's DRAM is used as the cache medium.  PMem  can also be used as the cache medium as it will provide a more cost effective solution for the requirements of a high performance environment.    Both indexing and caching as  Optimizer   Execution  are transparent for users. See the  Features  section for details.    Spark  ThriftServer * is a good use case for OAP, because ThriftServer launches Spark Applications which can cache hot data for a long time in the background, and it also accepts query requests from different clients at the same time.  bin/spark-sql ,  bin/spark-shell  or  bin/pyspark  can certainly be used by SQL Index and Data Source Cache, but usually only for interactive test situations.", 
            "title": "Architecture"
        }, 
        {
            "location": "/#features", 
            "text": "Use indexing and caching to improve Spark SQL performance on ad-hoc queries and batch processing jobs.", 
            "title": "Features"
        }, 
        {
            "location": "/#indexing", 
            "text": "Users can use SQL DDL(create/drop/refresh/check/show index) to use indexing. Once users create indices using DDL, index files are generated in a specific directory and mainly composed of index data and statistics. When queries are executed, analyzing index files to boost performance is transparent to users.    BTREE, BITMAP Index is an optimization that is widely used in traditional databases. We also adopt these two index types in the project. BTREE indexing is intended for datasets that have a lot of distinct values, and are distributed randomly, such as telephone numbers or ID numbers. BitMap index is intended for datasets with a limited total amount of distinct values, such as state or age.    Statistics are located in the index file after all the index data are written into the index file. Sometimes, reading indices could bring extra cost for some queries. So we also support 4 statistics (MinMax, Bloom Filter, SampleBase and PartByValue) to help filter. With statistics, we can make sure we only use indices if we can possibly improve the execution.", 
            "title": "Indexing"
        }, 
        {
            "location": "/#caching", 
            "text": "Caching is another core feature of OAP. It is also transparent to users. Data Source Cache can automatically load frequently queried (hot) data, and evict data automatically according to the LRU policy when cache is full. Data Source Cache has the following characteristics:   Off-Heap memory. The Data Source Cache uses off-heap memory and avoids the JVM GC. It can also use  PMem  as high-performance, high-capacity, low-cost memory  Cache-Locality. Data Source Cache can schedule computing tasks to the executor which holds needed data in cache, by implementing a cache aware mechanism based on Spark driver and executors communication.  Cache granularity. A column in one RowGroup (equivalent to Stripe in ORC) of a column-oriented storage format file is loaded into a basic cache unit which is called a \"Fiber\" in OAP.  Cache Eviction. Data Source Cache cache eviction uses LRU policy, and automatically caches and evicts data transparently to end user.  Cache configured tables. Data Source Cache also supports caching specific tables by configuring items according to actual situations, these tables are usually hot tables.", 
            "title": "Caching"
        }, 
        {
            "location": "/#42other-names-and-brands-may-be-claimed-as-the-property-of-others", 
            "text": "", 
            "title": "*Other names and brands may be claimed as the property of others."
        }, 
        {
            "location": "/OAP-Installation-Guide/", 
            "text": "OAP Installation Guide\n\n\nThis document introduces how to install OAP and its dependencies on your cluster nodes by \nConda\n. \nFollow steps below on \nevery node\n of your cluster to set right environment for each machine.\n\n\nContents\n\n\n\n\nPrerequisites\n\n\nInstalling OAP\n\n\nConfiguration\n\n\n\n\nPrerequisites\n\n\n\n\n\n\nOS Requirements\n\nWe have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use \nFedora 29 CentOS 7.6 or above\n. Besides, for \nMemkind\n we recommend you use \nkernel above 3.10\n.\n\n\n\n\n\n\nConda Requirements\n \n\nInstall Conda on your cluster nodes with below commands and follow the prompts on the installer screens.:\n\n\n\n\n\n\n$ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh\n$ chmod +x Miniconda2-latest-Linux-x86_64.sh \n$ bash Miniconda2-latest-Linux-x86_64.sh \n\n\n\n\nFor changes to take effect, close and re-open your current shell. To test your installation,  run the command \nconda list\n in your terminal window. A list of installed packages appears if it has been installed correctly.\n\n\nInstalling OAP\n\n\nDependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps.\n\n\n\n\nArrow\n\n\nPlasma\n\n\nMemkind\n\n\nVmemcache\n\n\nHPNL\n\n\nPMDK\n  \n\n\nOneAPI\n\n\n\n\nCreate a conda environment and install OAP Conda package.\n\n\n$ conda create -n oapenv -y python=3.7\n$ conda activate oapenv\n$ conda install -c conda-forge -c intel -y oap=1.0.0\n\n\n\n\nOnce finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under \n$HOME/miniconda2/envs/oapenv/oap_jars\n\n\nExtra Steps for PMem Shuffle\n\n\nIf you use one of OAP features -- \nPMem Shuffle\n with \nRDMA\n, you need to configure and validate RDMA, please refer to \nPMem Shuffle\n for the details.\n\n\nConfiguration\n\n\nOnce finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to \n$SPARK_HOME/conf/spark-defaults.conf\n.\n\n\nspark.executorEnv.LD_LIBRARY_PATH   $HOME/miniconda2/envs/oapenv/lib\nspark.executor.extraLibraryPath     $HOME/miniconda2/envs/oapenv/lib\nspark.driver.extraLibraryPath       $HOME/miniconda2/envs/oapenv/lib\nspark.executor.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar\nspark.driver.extraClassPath         $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar\n\n\n\n\nAnd then you can follow the corresponding feature documents for more details to use them.", 
            "title": "OAP Installation Guide"
        }, 
        {
            "location": "/OAP-Installation-Guide/#oap-installation-guide", 
            "text": "This document introduces how to install OAP and its dependencies on your cluster nodes by  Conda . \nFollow steps below on  every node  of your cluster to set right environment for each machine.", 
            "title": "OAP Installation Guide"
        }, 
        {
            "location": "/OAP-Installation-Guide/#contents", 
            "text": "Prerequisites  Installing OAP  Configuration", 
            "title": "Contents"
        }, 
        {
            "location": "/OAP-Installation-Guide/#prerequisites", 
            "text": "OS Requirements \nWe have tested OAP on Fedora 29 and CentOS 7.6 (kernel-4.18.16). We recommend you use  Fedora 29 CentOS 7.6 or above . Besides, for  Memkind  we recommend you use  kernel above 3.10 .    Conda Requirements   \nInstall Conda on your cluster nodes with below commands and follow the prompts on the installer screens.:    $ wget -c https://repo.continuum.io/miniconda/Miniconda2-latest-Linux-x86_64.sh\n$ chmod +x Miniconda2-latest-Linux-x86_64.sh \n$ bash Miniconda2-latest-Linux-x86_64.sh   For changes to take effect, close and re-open your current shell. To test your installation,  run the command  conda list  in your terminal window. A list of installed packages appears if it has been installed correctly.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/OAP-Installation-Guide/#installing-oap", 
            "text": "Dependencies below are required by OAP and all of them are included in OAP Conda package, they will be automatically installed in your cluster when you Conda install OAP. Ensure you have activated environment which you created in the previous steps.   Arrow  Plasma  Memkind  Vmemcache  HPNL  PMDK     OneAPI   Create a conda environment and install OAP Conda package.  $ conda create -n oapenv -y python=3.7\n$ conda activate oapenv\n$ conda install -c conda-forge -c intel -y oap=1.0.0  Once finished steps above, you have completed OAP dependencies installation and OAP building, and will find built OAP jars under  $HOME/miniconda2/envs/oapenv/oap_jars", 
            "title": "Installing OAP"
        }, 
        {
            "location": "/OAP-Installation-Guide/#extra-steps-for-pmem-shuffle", 
            "text": "If you use one of OAP features --  PMem Shuffle  with  RDMA , you need to configure and validate RDMA, please refer to  PMem Shuffle  for the details.", 
            "title": "Extra Steps for PMem Shuffle"
        }, 
        {
            "location": "/OAP-Installation-Guide/#configuration", 
            "text": "Once finished steps above, make sure libraries installed by Conda can be linked by Spark, please add the following configuration settings to  $SPARK_HOME/conf/spark-defaults.conf .  spark.executorEnv.LD_LIBRARY_PATH   $HOME/miniconda2/envs/oapenv/lib\nspark.executor.extraLibraryPath     $HOME/miniconda2/envs/oapenv/lib\nspark.driver.extraLibraryPath       $HOME/miniconda2/envs/oapenv/lib\nspark.executor.extraClassPath       $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar\nspark.driver.extraClassPath         $HOME/miniconda2/envs/oapenv/oap_jars/$OAP_FEATURE.jar  And then you can follow the corresponding feature documents for more details to use them.", 
            "title": "Configuration"
        }, 
        {
            "location": "/OAP-Developer-Guide/", 
            "text": "OAP Developer Guide\n\n\nThis document contains the instructions \n scripts on installing necessary dependencies and building OAP. \nYou can get more detailed information from OAP each module below.\n\n\n\n\nSQL DS Cache\n\n\nPMem Common\n\n\nPMem Spill\n\n\nPMem Shuffle\n\n\nRemote Shuffle\n\n\nOAP MLlib\n\n\nArrow Data Source\n\n\nNative SQL Engine\n\n\n\n\nBuilding OAP\n\n\nPrerequisites for Building\n\n\nOAP is built with \nApache Maven\n and Oracle Java 8, and mainly required tools to install on your cluster are listed below.\n\n\n\n\nCmake\n\n\nGCC \n 7\n\n\nMemkind\n\n\nVmemcache\n\n\nHPNL\n\n\nPMDK\n  \n\n\nOneAPI\n\n\n\n\nArrow\n\n\n\n\n\n\nRequirements for PMem Shuffle\n\n\n\n\n\n\nIf enable PMem Shuffle with RDMA, you can refer to \nPMem Shuffle\n to configure and validate RDMA in advance.\n\n\nWe provide scripts below to help automatically install dependencies above \nexcept RDMA\n, need change to \nroot\n account, run:\n\n\n# git clone -b \nversion\n https://github.com/oap-project/oap-tools.git\n# cd oap-tools\n# sh dev/install-compile-time-dependencies.sh\n\n\n\n\nRun the following command to learn more.\n\n\n# sh dev/scripts/prepare_oap_env.sh --help\n\n\n\n\nRun the following command to automatically install specific dependency such as Maven.\n\n\n# sh dev/scripts/prepare_oap_env.sh --prepare_maven\n\n\n\n\nBuilding\n\n\nTo build OAP package, run command below then you can find a tarball named \noap-$VERSION-bin-spark-$VERSION.tar.gz\n under directory \n$OAP_TOOLS_HOME/dev/release-package\n.\n\n\n$ sh $OAP_TOOLS_HOME/dev/compile-oap.sh\n\n\n\n\nBuilding Specified OAP Module, such as \nsql-ds-cache\n, run:\n\n\n$ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache", 
            "title": "OAP Developer Guide"
        }, 
        {
            "location": "/OAP-Developer-Guide/#oap-developer-guide", 
            "text": "This document contains the instructions   scripts on installing necessary dependencies and building OAP. \nYou can get more detailed information from OAP each module below.   SQL DS Cache  PMem Common  PMem Spill  PMem Shuffle  Remote Shuffle  OAP MLlib  Arrow Data Source  Native SQL Engine", 
            "title": "OAP Developer Guide"
        }, 
        {
            "location": "/OAP-Developer-Guide/#building-oap", 
            "text": "", 
            "title": "Building OAP"
        }, 
        {
            "location": "/OAP-Developer-Guide/#prerequisites-for-building", 
            "text": "OAP is built with  Apache Maven  and Oracle Java 8, and mainly required tools to install on your cluster are listed below.   Cmake  GCC   7  Memkind  Vmemcache  HPNL  PMDK     OneAPI   Arrow    Requirements for PMem Shuffle    If enable PMem Shuffle with RDMA, you can refer to  PMem Shuffle  to configure and validate RDMA in advance.  We provide scripts below to help automatically install dependencies above  except RDMA , need change to  root  account, run:  # git clone -b  version  https://github.com/oap-project/oap-tools.git\n# cd oap-tools\n# sh dev/install-compile-time-dependencies.sh  Run the following command to learn more.  # sh dev/scripts/prepare_oap_env.sh --help  Run the following command to automatically install specific dependency such as Maven.  # sh dev/scripts/prepare_oap_env.sh --prepare_maven", 
            "title": "Prerequisites for Building"
        }, 
        {
            "location": "/OAP-Developer-Guide/#building", 
            "text": "To build OAP package, run command below then you can find a tarball named  oap-$VERSION-bin-spark-$VERSION.tar.gz  under directory  $OAP_TOOLS_HOME/dev/release-package .  $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh  Building Specified OAP Module, such as  sql-ds-cache , run:  $ sh $OAP_TOOLS_HOME/dev/compile-oap.sh --sql-ds-cache", 
            "title": "Building"
        }
    ]
}